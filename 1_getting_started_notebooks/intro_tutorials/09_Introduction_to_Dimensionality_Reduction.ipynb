{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## Introduction to Dimensionality Reduction\n",
    "#### By Paul Hendricks\n",
    "-------\n",
    "\n",
    "In this notebook, we will show how to do GPU accelerated Dimensionality Reduction in RAPIDS.\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "* [Introduction to Dimensionality Reduction](#introduction)\n",
    "* [Principle Components Analysis](#pca)\n",
    "* [Truncated SVC](#tvsd)\n",
    "* [UMAP](#umap)\n",
    "* [Setup](#setup)\n",
    "* [Conclusion](#conclusion)\n",
    "\n",
    "Before going any further, let's make sure we have access to `matplotlib`, a popular Python library for visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import matplotlib\n",
    "except ModuleNotFoundError:\n",
    "    os.system('conda install -y matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Setup\n",
    "\n",
    "This notebook was tested using the following Docker containers:\n",
    "\n",
    "* `rapidsai/rapidsai-nightly:0.8-cuda10.0-devel-ubuntu18.04-gcc7-py3.7` from [DockerHub - rapidsai/rapidsai-nightly](https://hub.docker.com/r/rapidsai/rapidsai-nightly)\n",
    "\n",
    "This notebook was run on the NVIDIA Tesla V100 GPU. Please be aware that your system may be different and you may need to modify the code or install packages to run the below examples. \n",
    "\n",
    "If you think you have found a bug or an error, please file an issue here: https://github.com/rapidsai/notebooks/issues\n",
    "\n",
    "Before we begin, let's check out our hardware setup by running the `nvidia-smi` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see what CUDA version we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load some helper functions from `matplotlib` and configure the Jupyter Notebook for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data\"></a>\n",
    "## Data\n",
    "\n",
    "We'll start by loading the Digits dataset. This dataset is composed of 1,797 images of hand written digits, each of shape 8 x 8 (64 total columns when flattended) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; print('NumPy Version:', np.__version__)\n",
    "import sklearn; print('Scikit-Learn Version:', sklearn.__version__)\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits['data'], digits['target']\n",
    "X, y = X.astype(np.float32), y.astype(np.float32)\n",
    "print('X: ', X.shape, X.dtype, 'y: ', y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize several examples from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "figure = plt.figure()\n",
    "f, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "\n",
    "i = 0\n",
    "for row in axes:\n",
    "    for axis in row:\n",
    "        axis.imshow(X[i].reshape(8, 8), cmap='gray')\n",
    "        axis.set_title('Class: ' + str(int(y[i])))\n",
    "        i += 1\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pca\"></a>\n",
    "## Principle Components Analysis\n",
    "\n",
    "PCA (Principal Component Analysis) is a fundamental dimensionality reduction technique used to combine features in X in linear combinations such that each new component captures the most information or variance of the data. `n_components`is usually small, say at 3, where it can be used for data visualization, data compression and exploratory analysis.\n",
    "\n",
    "cuML’s `PCA` expects a cuDF DataFrame, and provides 2 algorithms Full and Jacobi. Full (default) uses a full eigendecomposition then selects the top K eigenvectors. The Jacobi algorithm is much faster as it iteratively tries to correct the top K eigenvectors, but might be less accurate.\n",
    "\n",
    "Before we fit a PCA model to the data, we'll standardize the data using Scikit-Learn's `StandardScaler` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit this object to the dataset using the `fit` and `transform` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the `PCA` class from Scikit-Learn and instantiate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit this model to our data and then use that estimated model to reduce our dataset to the 2 principle components that explain as much variation in the data as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_scaled)\n",
    "components = pca.transform(X_scaled)\n",
    "components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've reduced our dataset to 2 dimensions, we can easily visualize this dataset. The below code will plot each digit with its associated principle component values. Images within a class are visually similar; these images will appear within clusters. Additionally, classes that are visually similar (6s may look like 0s; 3s and 9s may look like 8s; etc.) are going to appear closer to each other. \n",
    "\n",
    "With this reduced dimensionality, it's easier to train an algorithm to classify an image into a class. Reducing the dimensionality of a dataset before building a classification or regression model is a common step in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set colors\n",
    "colors = ['blue', 'orange', 'green', 'red', 'purple', \n",
    "          'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "colors = ['tab:' + color for color in colors]\n",
    "\n",
    "# create figure\n",
    "figure = plt.figure()\n",
    "axis = figure.add_subplot(111)\n",
    "\n",
    "for i in range(10):\n",
    "    mask = y == i\n",
    "    axis.scatter(components[mask, 0], components[mask, 1], \n",
    "                 c=colors[i], label=str(i))\n",
    "axis.set_title('PCA (cpu)')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example used the Scikit-Learn implementation of PCA which is CPU-based. We can use a GPU accelerated version of this algorithm using the cuML library in RAPIDS.\n",
    "\n",
    "First, let's convert our data from a NumPy array representation to a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd; print('Pandas Version:', pd.__version__)\n",
    "\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled)\n",
    "X_scaled_df.columns = ['feature_' + str(i) for i in range(X_scaled_df.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert our Pandas DataFrame to a cuDF DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf; print('cuDF Version:', cudf.__version__)\n",
    "\n",
    "\n",
    "X_scaled_cudf = cudf.DataFrame.from_pandas(X_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Scikit-Learn, we can import a PCA model from cuML and instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml; print('cuML Version:', cuml.__version__)\n",
    "from cuml.decomposition import PCA as PCA_GPU\n",
    "\n",
    "\n",
    "pca_gpu = PCA_GPU(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit our PCA model to the data using the `fit` method and transform the dataset into principle components using the `transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_gpu.fit(X_scaled_cudf)\n",
    "components_gpu = pca_gpu.transform(X_scaled_cudf).to_pandas().values\n",
    "components_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the PCA components generated by the GPU model and compare them to those generated by the CPU model. They should be exactly the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "figure = plt.figure()\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "for i in range(10):\n",
    "    mask = y == i\n",
    "    ax1.scatter(components[mask, 0], components[mask, 1], \n",
    "                c=colors[i], label=str(i))\n",
    "    ax2.scatter(components_gpu[mask, 0], components_gpu[mask, 1], \n",
    "                c=colors[i], label=str(i))\n",
    "ax1.set_title('PCA (cpu)')\n",
    "ax2.set_title('PCA (gpu)')\n",
    "    \n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tvsd\"></a>\n",
    "## Truncated SVD\n",
    "\n",
    "TruncatedSVD is used to compute the top K singular values and vectors of a large matrix X. It is much faster when `n_components` is small, such as in the use of PCA when 3 components is used for 3D visualization.\n",
    "\n",
    "cuML’s `TruncatedSVD` expects a cuDF DataFrame, and provides 2 algorithms Full and Jacobi. Full (default) uses a full eigendecomposition then selects the top K singular vectors. The Jacobi algorithm is much faster as it iteratively tries to correct the top K singular vectors, but might be less accurate.\n",
    "\n",
    "Below, we import the `TruncatedSVD` from Scikit-Learn and instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "tsvd = TruncatedSVD(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we fitted the model using the `fit` method and used the model to transform the dataset into components using the `transform`. Using the `fit_transform`, we can accomplish both of these steps in one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = tsvd.fit_transform(X)\n",
    "components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we can visualize the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "figure = plt.figure()\n",
    "axis = figure.add_subplot(111)\n",
    "\n",
    "for i in range(10):\n",
    "    mask = y == i\n",
    "    axis.scatter(components[mask, 0], components[mask, 1], \n",
    "                 c=colors[i], label=str(i))\n",
    "axis.set_title('Truncated SVD (cpu)')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's accomplish the same workflow using cuDF and cuML. We'll first prepare our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X)\n",
    "X_df.columns = ['feature_' + str(i) for i in range(X_df.shape[1])]\n",
    "X_cudf = cudf.DataFrame.from_pandas(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll instantiate the GPU-accelerated version of `TruncatedSVD` from cuML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.decomposition import TruncatedSVD as TruncatedSVD_GPU\n",
    "\n",
    "\n",
    "tvsd_gpu = TruncatedSVD_GPU(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `fit_transform` method to fit the model and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_gpu = tvsd_gpu.fit_transform(X_cudf).to_pandas().values\n",
    "components_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll visualize the resulting components and show the CPU-based results and the GPU-based results side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "figure = plt.figure()\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "for i in range(10):\n",
    "    mask = y == i\n",
    "    ax1.scatter(components[mask, 0], components[mask, 1], \n",
    "                c=colors[i], label=str(i))\n",
    "    ax2.scatter(components_gpu[mask, 0], components_gpu[mask, 1], \n",
    "                c=colors[i], label=str(i))\n",
    "ax1.set_title('Truncated SVD (cpu)')\n",
    "ax2.set_title('Truncated SVD (gpu)')\n",
    "    \n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"umap\"></a>\n",
    "## UMAP\n",
    "\n",
    "Uniform Manifold Approximation and Projection Finds a low dimensional embedding of the data that approximates an underlying manifold.\n",
    "\n",
    "We'll import the `UMAP` class from cuML and instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import UMAP as UMAP_GPU\n",
    "\n",
    "\n",
    "umap_gpu = UMAP_GPU(n_neighbors=10, n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `fit_transform` method to fit the model and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_gpu = umap_gpu.fit_transform(X_cudf).to_pandas().values\n",
    "components_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll visualize the resulting components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create figure\n",
    "figure = plt.figure()\n",
    "axis = figure.add_subplot(111)\n",
    "\n",
    "for i in range(10):\n",
    "    mask = y == i\n",
    "    axis.scatter(components_gpu[mask, 0], components_gpu[mask, 1], \n",
    "                 c=colors[i], label=str(i))\n",
    "axis.set_title('UMAP (gpu)')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we showed how to do GPU accelerated Dimensionality Reduction in RAPIDS.\n",
    "\n",
    "To learn more about RAPIDS, be sure to check out: \n",
    "\n",
    "* [Open Source Website](http://rapids.ai)\n",
    "* [GitHub](https://github.com/rapidsai/)\n",
    "* [Press Release](https://nvidianews.nvidia.com/news/nvidia-introduces-rapids-open-source-gpu-acceleration-platform-for-large-scale-data-analytics-and-machine-learning)\n",
    "* [NVIDIA Blog](https://blogs.nvidia.com/blog/2018/10/10/rapids-data-science-open-source-community/)\n",
    "* [Developer Blog](https://devblogs.nvidia.com/gpu-accelerated-analytics-rapids/)\n",
    "* [NVIDIA Data Science Webpage](https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
