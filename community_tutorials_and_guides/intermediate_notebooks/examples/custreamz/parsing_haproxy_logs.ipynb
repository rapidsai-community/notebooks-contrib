{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HAProxy Logs\n",
    "\n",
    "This notebook demonstrates real world log parsing examples. For this notebook we will use the popular HAProxy\n",
    "format.\n",
    "\n",
    "This notebook will cover\n",
    "+ Streaming log entry event level logs from Kafka using Streamz\n",
    " - If using the [custreamz Docker image](https://github.com/rapidsai/notebooks-contrib/tree/master/intermediate_notebooks/examples/custreamz/BUILD.md) Kafka is installed and running for you by default.\n",
    " - If not using the provided Docker image users can refer to: https://kafka.apache.org/quickstart for reference on how to start a local Kafka cluster.\n",
    "+ Counting log word occurrences - (Word Count)\n",
    "+ Calculating average backend response time\n",
    "\n",
    "Ok so what do these HAProxy logs look like? Well here is an example.\n",
    "\n",
    "```{\"logline\": \"[haproxy@10.0.0.1] <134>May 29 19:08:36 haproxy[113498]: 45.26.605.15:38738 [29/May/2019:19:08:36.691] HTTPS:443~ HTTP_ProvisionManagers/mp3 4/5/0/1/1 200 6182 - - --NI 3/3/0/0/0 0/0 {|} \"GET /v2/serverinfo HTTP/1.1\"}```\n",
    "\n",
    "It is unlikely you have a Kafka topic with these log messages in it already so lets generate some sample messages for you. First we need to install a few required dependencies and define some global configurations however.\n",
    "\n",
    "Users using the [custreamz Docker image](https://github.com/rapidsai/notebooks-contrib/tree/master/intermediate_notebooks/examples/custreamz/BUILD.md) do not need to perform the following conda install since the container already has the needed dependencies. Running it again will not causes any problems but is not necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.6.14\n",
      "  latest version: 4.7.12\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge -c anaconda -y streamz python-confluent-kafka networkx graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create topci haproxy-logs: KafkaError{code=TOPIC_ALREADY_EXISTS,val=36,str=\"Topic 'haproxy-logs' already exists.\"}\n"
     ]
    }
   ],
   "source": [
    "import confluent_kafka as ck\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "num_messages_to_produce = 500000\n",
    "\n",
    "kafka_brokers = ['localhost:9092'] # This is a list of your Kafka brokers and ports.\n",
    "topic = 'haproxy-logs'\n",
    "\n",
    "kafka_conf = {'bootstrap.servers': kafka_brokers[0], 'group.id': 'custreamz', 'session.timeout.ms': 60000}  # Kafka configuration parameters. Any additional Kafka configurations can be placed here ...\n",
    "\n",
    "producer = ck.Producer(kafka_conf)  # Kafka producer\n",
    "\n",
    "# !!! - COMMENT OUT THE BELOW IF YOU WOULD NOT LIKE TO HAVE THIS SCRIPT CREATE A KAFKA TOPIC FOR YOU !!!\n",
    "admin = AdminClient({'bootstrap.servers': kafka_brokers[0]})\n",
    "fs = admin.create_topics([NewTopic(topic, 1, 1)], request_timeout=15.0)\n",
    "\n",
    "for topic, f in fs.items():\n",
    "    try:\n",
    "        f.result()\n",
    "        print(\"Topic {} created\".format(topic))\n",
    "    except Exception as e:\n",
    "        print(\"Failed to create topci {}: {}\".format(topic, e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sample HAProxy Data\n",
    "First thing is first. Lets generate some sample HAProxy logs into our Kafka environment so the following examples have something to pull from. This code has nothing to do with RAPIDS but rather just a simple script for generating sample HAProxy logs and publishing them into Kafka.\n",
    "\n",
    "This is certainly not the most efficient way to write to Kafka but it is the most simple for example purposes. Please be patient while the produce occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: 500000 sample HAProxy logs to Kafka topic: haproxy-logs at broker: localhost:9092\n",
      "0\n",
      "90000\n",
      "180000\n",
      "270000\n",
      "360000\n",
      "450000\n",
      "500000 - HAProxy logs written to Kafka\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from random import randrange\n",
    "\n",
    "sample_data = {}\n",
    "sample_data['log_ip'] = ['10.0.0.3', '14.2.3.4', '15.2.6.9', '10.2.34.6', '10.23.34.1']\n",
    "sample_data['syslog_timestamp'] = ['May 28 2019 00:00:09', 'May 28 2019 00:00:10', 'May 28 2019 00:00:11',\\\n",
    "                                   'May 28 2019 00:00:39','May 28 2019 00:00:51', 'May 28 2019 00:10:09']\n",
    "sample_data['program'] = ['haproxy']\n",
    "sample_data['pid'] = [113345, 756487, 352453, 352465, 164541]\n",
    "sample_data['client_ip'] = ['156.23.224.56', '126.52.74.15', '247.81.56.21', '26.245.255.1', '255.116.145.2']\n",
    "sample_data['client_port'] = [13345, 56487, 52453, 52465, 64541]\n",
    "sample_data['accept_date'] = ['28/May/2019:00:10:09.492', '28/May/2019:00:09:10.006', '28/May/2019:00:02:10.748',\\\n",
    "                              '28/May/2019:00:20:10.891', '28/May/2019:00:02:10.461', '28/May/2019:00:02:11.959']\n",
    "sample_data['frontend_name'] = ['px-http', 'https:443', 'tx-http']\n",
    "sample_data['server_name'] = ['srv1', 'srv2', 'srv3', 'srv4', 'srv5']\n",
    "sample_data['time_request'] = [0, 1, 2, 3]\n",
    "sample_data['time_queue'] = [0, 1, 2, 3]\n",
    "sample_data['time_backend_connect'] = [1, 2, 3]\n",
    "sample_data['time_backend_response'] = [2, 3, 4, 5, 6, 7, 8, 9]\n",
    "sample_data['time_duration'] = [13, 14, 16, 20, 23, 25]\n",
    "sample_data['http_status_code'] = [200, 400, 201, 401, 403]\n",
    "sample_data['bytes_read'] = [4, 573, 442, 234, 124, 1567]\n",
    "sample_data['captured_request'] = ['-']\n",
    "sample_data['captured_response'] = ['-']\n",
    "sample_data['termination_state'] = ['----', 'PH--', 'CR--', '--NI', '--SG']\n",
    "sample_data['actconn'] = [1, 2, 3, 4]\n",
    "sample_data['feconn'] = [2, 3, 5, 7, 8]\n",
    "sample_data['beconn'] = [0, 1, 2, 3, 4]\n",
    "sample_data['srvconn'] = [0, 1, 3]\n",
    "sample_data['retries'] = [0, 1, 2]\n",
    "sample_data['srv_queue'] = [0, 1, 2, 3]\n",
    "sample_data['backend_queue'] = [0, 2, 3, 4, 5, 7, 8, 9]\n",
    "\n",
    "cols = ['log_ip','syslog_timestamp','program','pid','client_ip','client_port',\\\n",
    "        'accept_date','frontend_name','backend_name','server_name','time_request',\\\n",
    "        'time_queue','time_backend_connect', 'time_backend_response', 'time_duration',\\\n",
    "        'http_status_code', 'bytes_read', 'captured_request', 'captured_response',\\\n",
    "        'termination_state','actconn','feconn','beconn','srvconn','retries','srv_queue','backend_queue']\n",
    "\n",
    "def generate_log():\n",
    "    log_skelton = \"[haproxy@{0}] <134>{1} {2}[{3}]: {4}:{5} [{6}] {7} {8}/{9} {10}/{11}/{12}/{13}/{14} {15} {16} {17} {18} {19} {20}/{21}/{22}/{23}/{24} {25}/{26}\"\n",
    "    values = []\n",
    "    for idx, col in enumerate(cols):\n",
    "        if col in sample_data:\n",
    "            value_list = sample_data[col]\n",
    "            values.append(value_list[randrange(len(value_list))])\n",
    "        else:\n",
    "            values.append(values[-1])\n",
    "    dict_out = {}    \n",
    "    dict_out[\"logline\"] = log_skelton.format(*values)\n",
    "    return json.dumps(dict_out)\n",
    "\n",
    "\n",
    "count = 0\n",
    "print(\"Generating: {} sample HAProxy logs to Kafka topic: {} at broker: {}\".format(num_messages_to_produce, topic, kafka_brokers[0]))\n",
    "try:\n",
    "    while count < num_messages_to_produce:\n",
    "        producer.produce(topic, generate_log())\n",
    "        \n",
    "        # Default max local buffer size is 100,000 so flush after 90,000\n",
    "        if count % 90000 == 0:\n",
    "            print(count)\n",
    "            producer.flush()\n",
    "        count = count + 1\n",
    "except KeyboardInterrupt:\n",
    "    sys.stderr.write('%% Aborted by user\\n')\n",
    "    \n",
    "producer.flush()\n",
    "print(\"{} - HAProxy logs written to Kafka\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Word Occurrences\n",
    "\n",
    "A common task associated with log monitoring is counting the occurrence of a \"word\" with in a line of text. Streamz makes it easy to do this and we will show below how to create a map function to examine the HAProxy logs and count the number of \"403\" \"http_status_code\" responses.\n",
    "\n",
    "If a user needs to examine raw text they can also make use of nvstrings (now custrings, the GPU-accelerated string manipulation library) to tokenise all the messages in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from streamz import Stream\n",
    "from streamz.dataframe import DataFrame\n",
    "import json\n",
    "\n",
    "# Create a Dask LocalCUDACluster\n",
    "import dask\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from distributed import Client\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "# Piece meal parsing of the HAProxy logs to cuDF. Ugly but gets the job done.\n",
    "def haproxy_lines_to_cudf(messages):\n",
    "    \n",
    "    # Take the batch of logs from kafka and combine them into line delimted string for GPU parsing\n",
    "    json_input_string = \"\\n\".join([msg.decode('utf-8') for msg in messages])\n",
    "    gdf = cudf.read_json(json_input_string, lines=True, engine='cudf')\n",
    "    \n",
    "    clean_df = gdf['logline'].str.split(' ')\n",
    "\n",
    "    clean_df['log_ip'] = clean_df[0].str.lstrip('[haproxy@').str.rstrip(']')\n",
    "    clean_df.drop_column(0)\n",
    "\n",
    "    clean_df[1] = clean_df[1].str.split('>')[1]\n",
    "    syslog_timestamp = clean_df[1].data.cat([clean_df[2].data, clean_df[3].data, clean_df[4].data], sep=' ')\n",
    "    clean_df['syslog_timestamp'] = cudf.Series(syslog_timestamp)\n",
    "    for col in [1,2,3,4]:\n",
    "        clean_df.drop_column(col)\n",
    "\n",
    "    program_pid_df = clean_df[5].str.split('[')\n",
    "    program_sr = program_pid_df[0]\n",
    "    pid_sr = program_pid_df[1]\n",
    "    clean_df['program'] = program_sr\n",
    "    clean_df['pid'] = pid_sr.str.rstrip(']:')\n",
    "    clean_df = clean_df.drop(labels=[5])\n",
    "    del program_pid_df\n",
    "\n",
    "    client_pid_port_df = clean_df[6].str.split(':')\n",
    "    clean_df['client_ip'], clean_df['client_port'] = client_pid_port_df[0], client_pid_port_df[1]\n",
    "    clean_df.drop_column(6)\n",
    "    del client_pid_port_df\n",
    "\n",
    "    clean_df['accept_date'] = clean_df[7].str.lstrip('[').str.rstrip(']')\n",
    "    clean_df.drop_column(7)\n",
    "\n",
    "    clean_df.rename({8: 'frontend_name'}, inplace=True)\n",
    "    backend_server_df = clean_df[9].str.split('/')\n",
    "    clean_df['backend_name'], clean_df['server_name'] = backend_server_df[0], backend_server_df[1]\n",
    "    clean_df.drop_column(9)\n",
    "\n",
    "    time_cols = ['time_request', 'time_queue', 'time_backend_connect', 'time_backend_response', 'time_duration']\n",
    "    time_df = clean_df[10].str.split('/')\n",
    "    for col_id, col_name in enumerate(time_cols):\n",
    "        clean_df[col_name] = time_df[col_id]\n",
    "    clean_df.drop_column(10)\n",
    "    del time_df\n",
    "\n",
    "    clean_df.rename({11: 'http_status_code'}, inplace=True)\n",
    "    clean_df.rename({12: 'bytes_read'}, inplace=True)\n",
    "    clean_df.rename({13: 'captured_request', 14: 'captured_response', 15: 'termination_state'}, inplace=True)\n",
    "\n",
    "    con_cols = ['actconn', 'feconn', 'beconn', 'srvconn', 'retries']\n",
    "    con_df = clean_df[16].str.split('/')\n",
    "    for col_id, col_name in enumerate(con_cols):\n",
    "        clean_df[col_name] = con_df[col_id]\n",
    "    clean_df.drop_column(16)\n",
    "    del con_df\n",
    "\n",
    "    q_df = clean_df[17].str.split('/')\n",
    "    clean_df['srv_queue'], clean_df['backend_queue'] = q_df[0], q_df[1]\n",
    "    clean_df.drop_column(17)\n",
    "    del q_df\n",
    "    \n",
    "    clean_df['time_backend_response'] = clean_df['time_backend_response'].astype('int')\n",
    "    clean_df['http_status_code'] = clean_df['http_status_code'].astype('int')\n",
    "    \n",
    "    # Return the cuDF dataframe so that other operations in the stream can use it\n",
    "    return clean_df\n",
    "    \n",
    "# We now use Streamz to create a Stream from Kafka by polling the topic every 1s.\n",
    "source = Stream.from_kafka_batched(topic, kafka_conf, npartitions=1, poll_interval='1s', asynchronous=True, dask=True)\n",
    "\n",
    "# When started this returns to us a RAPIDS cuDF\n",
    "sdf = source.map(haproxy_lines_to_cudf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above snippet defines a function for parsing a batch of HAProxy logs into a cuDF and also defining streamz code that will be used to invoke that function. Keep in mind the Stream is not running until we later invoke \"start\" so right now we are still just building up our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of 403 from the \"http_status_code\"\n",
    "def count_auth_errors(gdf):\n",
    "    auth_errors = gdf[gdf['http_status_code'] == 403].http_status_code.count()\n",
    "    return auth_errors\n",
    "\n",
    "# Count the number of 200 codes from \"http_status_code\". Of course it would be better to make a common function for this but being verbose for demonstration purposes.\n",
    "def count_ok_codes(gdf):\n",
    "    ok_codes = gdf[gdf['http_status_code'] == 200].http_status_code.count()\n",
    "    return ok_codes\n",
    "\n",
    "# Here we demonstrate splitting from the parent \"sdf\" stream source and branch out to different streams.\n",
    "auth_codes = sdf.map(count_auth_errors).gather().sink(lambda err_count: print(\"{:,} - 403 codes were encountered.\".format(err_count)))\n",
    "ok_codes = sdf.map(count_ok_codes).gather().sink(lambda ok_count: print(\"{:,} - 200 codes were encountered.\".format(ok_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Average Backend Response Time\n",
    "\n",
    "Our code above has already taken care of creating a [Streamz Streaming DataFrame](https://streamz.readthedocs.io/en/latest/dataframes.html) using RAPIDS cuDF for us. So no need to do that again. What we can do is reuse that object and inquire different questions on it with another stream without having to pull all of the messages from Kafka again (slowest part of pipeline). Lets take a look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_response(gdf):\n",
    "    return gdf['time_backend_response'].mean()\n",
    "\n",
    "auth_resps = sdf.map(calculate_avg_response).gather().sink(lambda avg_time: print(\"{:.2f} second(s) average backend response time\".format(avg_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finished defining our Stream we need to actually start the stream so that it will begin to pull messages from Kafka. Take note that since streamz is an async platform you might think the job has exited and left you with no output. This should return in a couple of seconds however. Lets start the stream!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.50 second(s) average backend response time\n",
      "99,853 - 200 codes were encountered.\n",
      "99,667 - 403 codes were encountered.\n"
     ]
    }
   ],
   "source": [
    "source.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup - Optional\n",
    "\n",
    "If you would like to remove the Kafka Topic used in this example then you can run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic haproxy-logs deleted\n"
     ]
    }
   ],
   "source": [
    "import confluent_kafka as ck\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "admin = AdminClient({'bootstrap.servers': kafka_brokers[0]})\n",
    "fs = admin.delete_topics([topic], operation_timeout=30)\n",
    "\n",
    "for topic, f in fs.items():\n",
    "    try:\n",
    "        f.result()  # The result itself is None\n",
    "        print(\"Topic {} deleted\".format(topic))\n",
    "    except Exception as e:\n",
    "        print(\"Failed to delete topic {}: {}\".format(topic, e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
