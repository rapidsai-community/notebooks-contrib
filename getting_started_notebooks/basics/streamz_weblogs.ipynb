{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Log Processing on GPUs\n",
    "\n",
    "Almost since the coining of the phrase \"big data\", log-processing has been a primary use-case for analytics platforms.\n",
    "\n",
    "Logs are *voluminous*:\n",
    "\n",
    "A single website visit can result in 10s to 100s of log entries, each with lengthy strings of duplicated client information.\n",
    "\n",
    "They're *complex*:\n",
    "Extracting user activities often requires combining multiple records by time and unique session identifier(s).\n",
    "\n",
    "They're *time-sensitive*:\n",
    "When something goes wrong, you need to know quickly.\n",
    "\n",
    "While early big data architectures were oriented towards batch jobs, the focus has shifted to lower-latency solutions. Distributed data processing tools and APIs have made it easier for developers to write _streaming_ applications.\n",
    "\n",
    "Below we provide an example of how to do streaming web-log processing with RAPIDS, Dask, and Streamz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Requisites\n",
    "\n",
    "We assume you're running in a RAPIDS nightly or release container, and thus already have cuDF and Dask installed.\n",
    "\n",
    "Make sure you have [streamz](https://github.com/python-streamz/streamz) installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y streamz ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "For demonstration purposes, we'll use a [publicly available web-log dataset from NASA](http://opensource.indeedeng.io/imhotep/docs/sample-data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request, gzip, shutil\n",
    "\n",
    "data_dir = '/data/'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "url = 'http://indeedeng.github.io/imhotep/files/nasa_19950630.22-19950728.12.tsv.gz'\n",
    "fn = 'logs_noheader.tsv'\n",
    "\n",
    "if not os.path.isfile(data_dir+\"logs.tsv\"):\n",
    "    urllib.request.urlretrieve(url, data_dir+'logs.tsv'+'.gz')\n",
    "    with gzip.open(data_dir+'logs.tsv'+'.gz', 'r') as f_in, open(data_dir+'logs.tsv', 'wb') as f_out:\n",
    "      shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "# We remove the header line to avoid sending it in some batches and not others    \n",
    "with open(data_dir+\"logs.tsv\", 'rb') as fin:\n",
    "    # This is a latin character set so we must re-encode it\n",
    "    data = fin.read().decode('iso-8859-1').encode('utf8').splitlines(True)\n",
    "    names = str(data[0].decode('UTF-8')).split('\\t')\n",
    "with open(data_dir+\"logs_noheader.tsv\", 'wb') as fout:\n",
    "    fout.writelines(data[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Data\n",
    "\n",
    "The Google SRE HandBook says it's a good idea to track the [4 Golden Signals](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/#xref_monitoring_golden-signals) for any important system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host</th>\n",
       "      <th>logname</th>\n",
       "      <th>time</th>\n",
       "      <th>method</th>\n",
       "      <th>url</th>\n",
       "      <th>response</th>\n",
       "      <th>bytes</th>\n",
       "      <th>referer</th>\n",
       "      <th>useragent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199.72.81.55</td>\n",
       "      <td>-</td>\n",
       "      <td>804571201</td>\n",
       "      <td>GET</td>\n",
       "      <td>/history/apollo/</td>\n",
       "      <td>200</td>\n",
       "      <td>6245</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unicomp6.unicomp.net</td>\n",
       "      <td>-</td>\n",
       "      <td>804571206</td>\n",
       "      <td>GET</td>\n",
       "      <td>/shuttle/countdown/</td>\n",
       "      <td>200</td>\n",
       "      <td>3985</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>199.120.110.21</td>\n",
       "      <td>-</td>\n",
       "      <td>804571209</td>\n",
       "      <td>GET</td>\n",
       "      <td>/shuttle/missions/sts-73/mission-sts-73.html</td>\n",
       "      <td>200</td>\n",
       "      <td>4085</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>burger.letters.com</td>\n",
       "      <td>-</td>\n",
       "      <td>804571211</td>\n",
       "      <td>GET</td>\n",
       "      <td>/shuttle/countdown/liftoff.html</td>\n",
       "      <td>304</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199.120.110.21</td>\n",
       "      <td>-</td>\n",
       "      <td>804571211</td>\n",
       "      <td>GET</td>\n",
       "      <td>/shuttle/missions/sts-73/sts-73-patch-small.gif</td>\n",
       "      <td>200</td>\n",
       "      <td>4179</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   host logname       time method  \\\n",
       "0          199.72.81.55       -  804571201    GET   \n",
       "1  unicomp6.unicomp.net       -  804571206    GET   \n",
       "2        199.120.110.21       -  804571209    GET   \n",
       "3    burger.letters.com       -  804571211    GET   \n",
       "4        199.120.110.21       -  804571211    GET   \n",
       "\n",
       "                                               url  response  bytes  referer  \\\n",
       "0                                 /history/apollo/       200   6245       -1   \n",
       "1                              /shuttle/countdown/       200   3985       -1   \n",
       "2     /shuttle/missions/sts-73/mission-sts-73.html       200   4085       -1   \n",
       "3                  /shuttle/countdown/liftoff.html       304      0       -1   \n",
       "4  /shuttle/missions/sts-73/sts-73-patch-small.gif       200   4179       -1   \n",
       "\n",
       "   useragent\\n  \n",
       "0           -1  \n",
       "1           -1  \n",
       "2           -1  \n",
       "3           -1  \n",
       "4           -1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cudf\n",
    "\n",
    "df = cudf.read_csv(data_dir+fn, sep='\\t', names=names)\n",
    "df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data above doesn't tell us anything about request latency, but we can aggregate it to get a view into traffic, errors, and saturation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "007.thegap.com                     34\n",
       "01-dynamic-c.wokingham.luna.net    12\n",
       "02-dynamic-c.wokingham.luna.net    13\n",
       "03-dynamic-c.wokingham.luna.net    12\n",
       "04-dynamic-c.rotterdam.luna.net    22\n",
       "Name: host, dtype: int32"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate total requests served per host system\n",
    "traffic = df.groupby(['host']).host.count()\n",
    "traffic[traffic > 5].head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "host                     response\n",
       "129.130.115.19           501          1\n",
       "134.57.9.77              501          6\n",
       "163.205.1.45             500         53\n",
       "163.205.16.23            501          1\n",
       "192.83.171.94            501          1\n",
       "cc.newcastle.edu.au      501          1\n",
       "n1032036.ksc.nasa.gov    501          1\n",
       "newcastle03.nbnet.nb.ca  501          2\n",
       "titan02                  500          4\n",
       "titan02f                 500          4\n",
       "Name: host, dtype: int32"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count HTTP error codes per host system\n",
    "errors = df[df['response'] >= 500].groupby(['host', 'response']).host.count()\n",
    "errors.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163.206.137.21          109.174076\n",
       "alyssa.prodigy.com      142.961911\n",
       "news.ti.com             128.065425\n",
       "piweba1y.prodigy.com    197.037434\n",
       "piweba2y.prodigy.com    115.076846\n",
       "Name: bytes, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# measure possible saturation of host network cards\n",
    "mb_sent = df.groupby(['host']).bytes.sum()/1000000\n",
    "mb_sent[mb_sent > 100].head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the above that there are not many errors which is great and we can also see hits per host and total MBs sent per host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single GPU Streaming with RAPIDS and Streamz\n",
    "\n",
    "A single GPU can process a lot of data quickly. Thanks to the Streamz API, it's also easy to do it in streaming fashion.\n",
    "\n",
    "In many streaming systems you return events of interest for ops teams to investigate. That is what we will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "# calculate traffic, errors, and saturation per batch\n",
    "def process_on_gpu(messages):\n",
    "    df = cudf.read_csv(StringIO('\\n'.join(messages)), sep='\\t', names=names)\n",
    "    traffic = df.groupby(['host']).host.count()\n",
    "    errors = df[df['response'] >= 500].groupby(['host', 'response']).host.count()\n",
    "    mb_sent = df.groupby(['host']).bytes.sum()/1000000\n",
    "    \n",
    "    # Return - TSV versions of each metric\n",
    "    return {'traffic': str(traffic[traffic > 200]), 'errors': str(errors), 'mb_sent': str(mb_sent[mb_sent > 120])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "\n",
    "# save each metric type to its own file, instead of dumping lots of output to Jupyter\n",
    "def save_to_file(events):\n",
    "    dt = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    with open('/data/traffic.txt', 'w+') as fp:\n",
    "        fp.write(str(dt) + ':' + events['traffic'])\n",
    "    with open('/data/errors.txt', 'w+') as fp:\n",
    "        fp.write(str(dt) + ':' + events['errors'])\n",
    "    with open('/data/mb_sent.txt', 'w+') as fp:\n",
    "        fp.write(str(dt) + ':' + events['mb_sent'])\n",
    "    print(str(dt) + ': metrics batch written..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamz import Stream\n",
    "\n",
    "# setup the stream\n",
    "source = Stream()\n",
    "# process 250k lines per batch\n",
    "out = source.partition(250000).map(process_on_gpu).sink(save_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-18 20:28:21: metrics batch written..\n"
     ]
    }
   ],
   "source": [
    "# stream data from file. This is a slow means of emitting data to a stream\n",
    "# see below for a faster approach with Kafka\n",
    "with open(data_dir+fn, 'rb') as fp:\n",
    "    for line in fp.readlines():\n",
    "        source.emit(line.decode('iso-8859-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"Error Log:\"\n",
    "!head /data/errors.txt\n",
    "!echo \"\\nTraffic Log:\"\n",
    "!head /data/traffic.txt\n",
    "!echo \"\\nMB Sent Log:\"\n",
    "!head /data/mb_sent.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Streamz to multiple GPUs with Dask & Kafka\n",
    "\n",
    "As opposed to streaming from files a very common pattern is to read from distributed log systems like Apache Kafka.\n",
    "\n",
    "The below example assumes you have a running Kafka instance/cluster.\n",
    "\n",
    "For help setting up your own, follow the [Kafka Quickstart guide](http://kafka.apache.org/quickstart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "# create a Dask cluster with 1 worker per GPU\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamz import Stream\n",
    "import confluent_kafka\n",
    "\n",
    "# Kafka specific configurations\n",
    "topic = \"haproxy-topic\"\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "consumer_conf = {'bootstrap.servers': bootstrap_servers, 'group.id': 'custreamz', 'session.timeout.ms': 60000}\n",
    "\n",
    "stream = Stream.from_kafka_batched(topic, consumer_conf, poll_interval='1s', npartitions=1, asynchronous=True, dask=False)\n",
    "final_output = stream.map(process_on_gpu).sink(print)\n",
    "stream.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
