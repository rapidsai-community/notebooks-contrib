{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Datasets in Rapids\n",
    "\n",
    "Rapids not only offers the capability of handling static datasets but also streaming datasets. Performing data science on streaming datasets offers several challenges that are not presented when working with static datasets. This is the inherit nature of working with data that might be coming from several different sources. Each of those sources generally have different SLAs and performance levels which make things complicated for the downstream consumer who might be interested in using several of those sources. Likewise the consumer would need to handle late arriving data, windowing of the data, and transformations of the data from the different sources.\n",
    "\n",
    "As you can see this is no trivial effort. For these reasons Rapids has adopted Streamz to help alleviate these problems for our users. Streamz offers the benefits of allowing consumer to setup data pipelines to manage the complexities involved in streaming data more easily. This is why Rapids has chosen to use Streamz for managing streaming datasets. Lets take a look at whats involved to use Streams with Rapids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Streamz with Conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y streamz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Randy to put some BS here\n",
    "\n",
    "### Basic Usage\n",
    "\n",
    "When used in conjuntion with Rapids/cuDF Streamz offers an impressive list of features that can sit on top of \n",
    "Rapids while taking advantage of the increased computational efficiency of using GPUs that Rapids offers.\n",
    "\n",
    "Lets take a look at a simple example of passing data from Streamz to cuDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0.0.3    2057\n",
      "10.2.34.6    1965\n",
      "10.23.34.1    2070\n",
      "14.2.3.4    1978\n",
      "15.2.6.9    1930\n",
      "Name: 0, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "from streamz import Stream\n",
    "import cudf\n",
    "\n",
    "with open (\"haproxy_data.txt\", \"r\") as myfile:\n",
    "    data=myfile.readlines()\n",
    "\n",
    "def gpu_preprocess_simple_agg(messages):\n",
    "    json_input_string = \"\\n\".join([msg for msg in messages])\n",
    "    gdf = cudf.read_json(json_input_string, lines=True)\n",
    "    tmp = gdf['log_ip'].str.split()\n",
    "    ips = tmp[tmp.columns[0]]\n",
    "    \n",
    "    for ip in tmp.columns[1:]:\n",
    "        ips = ips.append(tmp[ip])\n",
    "    \n",
    "    return str(ips.groupby(ips).count())\n",
    "\n",
    "source = Stream()\n",
    "source.partition(10000).map(gpu_preprocess_simple_agg).sink(print)\n",
    "\n",
    "for line in data:\n",
    "    source.emit(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming Data from Kafka with Streamz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0.0.3    2057\n",
      "10.2.34.6    1965\n",
      "10.23.34.1    2070\n",
      "14.2.3.4    1978\n",
      "15.2.6.9    1930\n",
      "Name: 0, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "from streamz import Stream\n",
    "import cudf, io, json, confluent_kafka\n",
    "\n",
    "# Kafka specific configurations\n",
    "topic = \"haproxy-topic\"\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "consumer_conf = {'bootstrap.servers': bootstrap_servers, 'group.id': 'custreamz', 'session.timeout.ms': 60000}\n",
    "\n",
    "def gpu_preprocess_simple_agg(messages):\n",
    "    json_input_string = \"\\n\".join([msg.decode('utf-8') for msg in messages])\n",
    "    gdf = cudf.read_json(json_input_string, lines=True)\n",
    "    tmp = gdf['log_ip'].str.split()\n",
    "    ips = tmp[tmp.columns[0]]\n",
    "    \n",
    "    for ip in tmp.columns[1:]:\n",
    "        ips = ips.append(tmp[ip])\n",
    "    \n",
    "    return str(ips.groupby(ips).count())\n",
    "\n",
    "stream = Stream.from_kafka_batched(topic, consumer_conf, poll_interval='1s', npartitions=1, asynchronous=True, dask=False)\n",
    "final_output = stream.map(gpu_preprocess_simple_agg).sink(print)\n",
    "stream.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
