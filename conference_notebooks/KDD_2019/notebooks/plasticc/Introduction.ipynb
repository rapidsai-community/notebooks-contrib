{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAsTiCC Astronomical Classification Tutorial\n",
    "\n",
    "The Large Synoptic Survey Telescope (LSST) is increasing our ability to discover 10 to 100 times more astronomical sources in the\n",
    "night sky than can be seen with our eyes.\n",
    "\n",
    "The Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC) asked Kagglers to classify the data in an LSST survey of data.\n",
    "\n",
    "This tutorial will explore the [8th place](https://www.kaggle.com/c/PLAsTiCC-2018/leaderboard) solution to the challenge, which uses the RAPIDS open-source ecosystem of data science tools for GPU-accelerated feature engineering and model building. It has been simplified from the [original solution](https://devblogs.nvidia.com/make-sense-universe-rapids-ai/) in order to fit within a 40-minute tutorial session. \n",
    "\n",
    "The diagram below outlines the full end-to-end solution, which uses RAPIDS cuDF to engineer features for training XGBoost, MLP, and Bidirectional (attentional) RNN models. This tutorial will include the feature engineering and training processes for the XGBoost and RNN models. The training of the MLP model and the [model stacking](https://towardsdatascience.com/automate-stacking-in-python-fc3e7834772e) are left as future work for the students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://devblogs.nvidia.com/wp-content/uploads/2019/02/1tv0fuN-usAsQLJzTj1zSLQ.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of time, we will be implementing a simplified version of the end-to-end Kaggle solution. In this simplified version will, we will\n",
    "1. embed timeseries features into a 16-dimensional space using a pre-trained RNN model\n",
    "2. engineer aggregated features by building statistical summaries from features in the input data, and \n",
    "3. train an XGBoost classifier using both our embedded & statistical features to classify the data in the test set\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This tutorial is broken down into three parts:\n",
    "\n",
    "#### Part 1 - Training RNN Timeseries Embeddings\n",
    "- **Part 1.1:** Construct & analyzes timeseries features to train RNN\n",
    "- **Part 1.2:** Extracts bottleneck features from part 1.1\n",
    "\n",
    "#### Part 2 - Training an XGBoost Classifier\n",
    "- **Part 2.1:** Builds statistical features by aggregating different features together\n",
    "- **Part 2.2:** Uses features from Parts *1.2* and *2.1* to train and evaluate an XGBoost classifier\n",
    "\n",
    "#### Part 3 - Practice your understanding\n",
    "- **Part 3.1:** Contains a lab that can be worked independently to become more familiar with the RAPIDS ecosystem\n",
    "- **Part 3.2:** Contains detailed answers to the problems in *Part 3.1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "By the end of this tutorial, the student should feel comfortable doing common data processing tasks on GPUs using cuDF, training models in XGBoost, and integrating cuDF-built datasets into common deep learning frameworks like [Tensorflow](https://www.tensorflow.org/). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuml4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
