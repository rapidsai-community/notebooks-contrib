{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask_cudf\n",
    "import distributed\n",
    "import dask_xgboost as dxgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- setting dask settings\")\n",
    "dask.config.set({'distributed.scheduler.work-stealing': False})\n",
    "dask.config.set({'distributed.scheduler.bandwidth': 1})\n",
    "\n",
    "print(\"-- Changes to dask settings\")\n",
    "print(\"--- Setting work-stealing to \", dask.config.get('distributed.scheduler.work-stealing'))\n",
    "print(\"--- Setting scheduler bandwidth to \", dask.config.get('distributed.scheduler.bandwidth'))\n",
    "print(\"-- Settings updates complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = socket.gethostbyname(socket.gethostname())\n",
    "scheduler = \"tcp://\" + ip + \":8786\"\n",
    "client = distributed.Client(scheduler)\n",
    "client.restart()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update this path to reflect the datastore from Tracked Metrics if you downloaded the NYC Taxi Trip dataset\n",
    "datastore = \"/path/to/azure/datastore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names that need to be re-mapped\n",
    "remap = {}\n",
    "remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "#create a list of columns & dtypes the df must have\n",
    "must_haves = {\n",
    "    'pickup_datetime': 'datetime64[ms]',\n",
    "    'dropoff_datetime': 'datetime64[ms]',\n",
    "    'passenger_count': 'int32',\n",
    "    'trip_distance': 'float32',\n",
    "    'pickup_longitude': 'float32',\n",
    "    'pickup_latitude': 'float32',\n",
    "    'rate_code': 'int32',\n",
    "    'dropoff_longitude': 'float32',\n",
    "    'dropoff_latitude': 'float32',\n",
    "    'fare_amount': 'float32'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function which takes a DataFrame partition\n",
    "def clean(df_part, remap, must_haves):    \n",
    "    # some col-names include pre-pended spaces remove & lowercase column names\n",
    "    tmp = {col:col.strip().lower() for col in list(df_part.columns)}\n",
    "    df_part = df_part.rename(tmp)\n",
    "    \n",
    "    # rename using the supplied mapping\n",
    "    df_part = df_part.rename(remap)\n",
    "    \n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col)\n",
    "            continue\n",
    "\n",
    "        if df_part[col].dtype == 'object' and col in ['pickup_datetime', 'dropoff_datetime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "            \n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "    \n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust this dictionary list if you'd like to use different year in this workload\n",
    "is_valid_years = {\n",
    "    \"2014\": False,\n",
    "    \"2015\": False,\n",
    "    \"2016\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(datastore, \"data/nyctaxi\")\n",
    "\n",
    "dfs = []\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"WARNING: the NYC Taxi Trip Data was not found in the Azure datastore\")\n",
    "    print(\"WARNING: updating the data path to use a public datastore\")\n",
    "    print(\"WARNING: data will be downloaded and processed in-situ\")\n",
    "    print(\"WARNING: this degrades performance\")\n",
    "    print(\"WARNING: to avoid this performance degradation, use the `--download_nyctaxi_data=True` option when using start_azureml.py\")\n",
    "    data_path = \"gcs://anaconda-public-data/nyc-taxi/csv/\"\n",
    "    if is_valid_years[\"2014\"]:\n",
    "        taxi_df_2014 = dask_cudf.read_csv(os.path.join(data_path, \"2014/yellow_*.csv\"))\n",
    "        taxi_df_2014 = taxi_df_2014.map_partitions(clean, remap, must_haves)\n",
    "        dfs.append(taxi_df_2014)\n",
    "    if is_valid_years[\"2015\"]:\n",
    "        taxi_df_2015 = dask_cudf.read_csv(os.path.join(data_path, \"2015/yellow_*.csv\"))\n",
    "        taxi_df_2015 = taxi_df_2015.map_partitions(clean, remap, must_haves)\n",
    "        dfs.append(taxi_df_2015)\n",
    "    if is_valid_years[\"2016\"]:\n",
    "        valid_months_2016 = [str(x).rjust(2, '0') for x in range(1, 7)]\n",
    "        valid_files_2016 = [os.path.join(data_path, \"2016/yellow_tripdata_2016-{}.csv\".format(month)) for month in valid_months_2016]\n",
    "        taxi_df_2016 = dask_cudf.read_csv(valid_files_2016)\n",
    "        taxi_df_2016 = taxi_df_2016.map_partitions(clean, remap, must_haves)\n",
    "        dfs.append(taxi_df_2016)\n",
    "else:\n",
    "    if is_valid_years[\"2014\"] and os.path.exists(os.path.join(data_path, \"2014\")):\n",
    "        taxi_df_2014 = dask_cudf.read_csv(os.path.join(data_path, \"2014/yellow_*.csv\"))\n",
    "        taxi_df_2014 = taxi_df_2014.map_partitions(clean, remap, must_haves)\n",
    "        dfs.append(taxi_df_2014)\n",
    "    if is_valid_years[\"2015\"] and os.path.exists(os.path.join(data_path, \"2014\")):\n",
    "        taxi_df_2015 = dask_cudf.read_csv(os.path.join(data_path, \"2015/yellow_*.csv\"))\n",
    "        taxi_df_2015 = taxi_df_2015.map_partitions(clean, remap, must_haves)\n",
    "        dfs.append(taxi_df_2015)\n",
    "    if is_valid_years[\"2016\"] and os.path.exists(os.path.join(data_path, \"2014\")):\n",
    "        taxi_df_2016 = dask_cudf.read_csv(os.path.join(data_path, \"2016/yellow_*.csv\"))\n",
    "        taxi_df_2016 = taxi_df_2016.map_partitions(clean, remap, must_haves)\n",
    "        dfs.append(taxi_df_2016)\n",
    "\n",
    "taxi_df = dask.dataframe.multi.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column names are as follows:\")\n",
    "for column in taxi_df.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a list of filter conditions to throw out records with missing or outlier values\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]\n",
    "taxi_df = taxi_df.query(' and '.join(query_frags))\n",
    "\n",
    "# inspect the results of cleaning\n",
    "taxi_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "import numpy as np\n",
    "\n",
    "def haversine_distance_kernel(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, h_distance):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude)):\n",
    "        x_1 = pi / 180 * x_1\n",
    "        y_1 = pi / 180 * y_1\n",
    "        x_2 = pi / 180 * x_2\n",
    "        y_2 = pi / 180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat / 2)**2 + cos(x_1) * cos(x_2) * sin(dlon / 2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "        h_distance[i] = c * r\n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] < 3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m * 2.6) + y + (y // 4) + (c // 4) - 2 * c) % 7\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day'] = df['pickup_datetime'].dt.day\n",
    "    df['diff'] = df['dropoff_datetime'].astype('int32') - df['pickup_datetime'].astype('int32')\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['pickup_latitude'] // .01 * .01\n",
    "    df['pickup_longitude_r'] = df['pickup_longitude'] // .01 * .01\n",
    "    df['dropoff_latitude_r'] = df['dropoff_latitude'] // .01 * .01\n",
    "    df['dropoff_longitude_r'] = df['dropoff_longitude'] // .01 * .01\n",
    "    \n",
    "    df = df.drop('pickup_datetime')\n",
    "    df = df.drop('dropoff_datetime')\n",
    "\n",
    "    df = df.apply_rows(haversine_distance_kernel,\n",
    "                       incols=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'],\n",
    "                       outcols=dict(h_distance=np.float32),\n",
    "                       kwargs=dict())\n",
    "\n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                       incols=['day', 'month', 'year'],\n",
    "                       outcols=dict(day_of_week=np.float32),\n",
    "                       kwargs=dict())\n",
    "\n",
    "\n",
    "    df['is_weekend'] = (df['day_of_week']<2).astype(\"int32\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# actually add the features\n",
    "taxi_df = taxi_df.map_partitions(add_features).persist()\n",
    "done = distributed.wait(taxi_df)\n",
    "# inspect the result\n",
    "# taxi_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "taxi_df.groupby('hour').fare_amount.mean().compute().to_pandas().sort_index().plot(legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_train = taxi_df.query('day < 25').persist()\n",
    "\n",
    "# create a Y_train ddf with just the target variable\n",
    "Y_train = X_train[['fare_amount']].persist()\n",
    "# drop the target variable from the training ddf\n",
    "X_train = X_train[X_train.columns.difference(['fare_amount'])]\n",
    "\n",
    "# this wont return until all data is in GPU memory\n",
    "done = distributed.wait([X_train, Y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "    'learning_rate'  : 0.3,\n",
    "    'max_depth'      : 8,\n",
    "    'objective'      : 'reg:squarederror',\n",
    "    'subsample'      : 0.6,\n",
    "    'gamma'          : 1,\n",
    "    'silent'         : True,\n",
    "    'verbose_eval'   : True,\n",
    "    'tree_method'    :'gpu_hist'\n",
    "}\n",
    "\n",
    "trained_model = dxgb.train(client, params, X_train, Y_train, num_boost_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_partitions(df):\n",
    "    lengths = df.map_partitions(len).compute()\n",
    "    nonempty = [length > 0 for length in lengths]\n",
    "    return df.partitions[nonempty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = taxi_df.query('day >= 25').persist()\n",
    "X_test = drop_empty_partitions(X_test)\n",
    "\n",
    "# Create Y_test with just the fare amount\n",
    "Y_test = X_test[['fare_amount']]\n",
    "\n",
    "# Drop the fare amount from X_test\n",
    "X_test = X_test[X_test.columns.difference(['fare_amount'])]\n",
    "\n",
    "# display test set size\n",
    "# len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on the test set\n",
    "\n",
    "Y_test['prediction'] = dxgb.predict(client, trained_model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test['squared_error'] = (Y_test['prediction'] - Y_test['fare_amount'])**2\n",
    "\n",
    "# inspect the results to make sure our calculation looks right\n",
    "Y_test.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the actual RMSE over the full test set\n",
    "RMSE = Y_test.squared_error.mean().compute()\n",
    "math.sqrt(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
