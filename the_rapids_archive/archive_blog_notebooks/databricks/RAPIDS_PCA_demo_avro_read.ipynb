{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Accelerated Principal Component Analysis (PCA) using RAPIDS on a Sample Dataset with CPU vs GPU comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying GPUs\n",
    "\n",
    "RAPIDS requires GPUs with Pascal Architecture or better. That means any GPUs starting with K (Kepler) series (e.g. K80) or M (Maxwell) will not work with RAPIDS. You can use the `nvidia-smi` command to verify the type of your GPU as well as the memory size which may be needed for some of the RAPIDS examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's begin by importing RAPIDS and scikit learn libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA as skPCA\n",
    "from cuml import PCA as cumlPCA\n",
    "import cudf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "For this example we are downloading a sample dataset (Mortgage.csv) from Nvidia's repository\n",
    "\n",
    "__We have already completed Data prep (ETL) and feature engineering on this dataset and the dataset is ready for Machine Learning__\n",
    "\n",
    "We're going to first visually check the directory to see if you already have the dataset __\"mortgage_data.avro__\" because it takes some time to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls /dbfs/RAPIDS/mortgage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ If you already have a dataset, please go to the __\"Loading the data with Spark\" section__.  If you __don't__ have the dataset, please run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh \n",
    "\n",
    "wget https://github.com/rapidsai/notebooks-extended/raw/master/data/mortgage_data_tar.tar.gz\n",
    "\n",
    "tar -xzf mortgage_data_tar.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything downloaded and extracted ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a directory and copy the extracted file there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "mkdir -p /dbfs/RAPIDS/mortgage\n",
    "rm -rf /dbfs/RAPIDS/mortgage/mortgage_data.avro\n",
    "mv mortgage_data.avro /dbfs/RAPIDS/mortgage/mortgage_data.avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format(\"avro\").load(\"/RAPIDS/mortgage/mortgage_data.avro/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Helper Functions to remove any null values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def recast(df):\n",
    "   for column, data_type in df.dtypes:\n",
    "       if str(data_type) == \"string\":\n",
    "           df = df.withColumn(column, df[column].cast(\"float\"))\n",
    "   return df\n",
    "\n",
    "data = recast(data).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's check out the dataset in the spark dataframe and count the number of rows in the dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCount = data.count() # We're storing this value for later use\n",
    "print(dataCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Helper Functions to compare CPU vs GPU results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def array_equal(a,b,threshold=2e-3,with_sign=True):\n",
    "    a = to_nparray(a)\n",
    "    b = to_nparray(b)\n",
    "    if with_sign == False:\n",
    "        a,b = np.abs(a),np.abs(b)\n",
    "    error = mean_squared_error(a,b)\n",
    "    res = error<threshold\n",
    "    return res\n",
    "\n",
    "def to_nparray(x):\n",
    "    if isinstance(x,np.ndarray) or isinstance(x,pd.DataFrame):\n",
    "        return np.array(x)\n",
    "    elif isinstance(x,np.float64):\n",
    "        return np.array([x])\n",
    "    elif isinstance(x,cudf.DataFrame) or isinstance(x,cudf.Series):\n",
    "        return x.to_pandas().values\n",
    "    return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the Spark Dataframe into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load data function allows you to create a user defined sample of your data and converts the spark dataframe to pandas dataframe.  Then, it removes any null values in the dataset.  If you want to to experiment with a different dataset sizes, use the random array generator to load the random data.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(nrows, ncols):\n",
    "  try:\n",
    "    frac = nrows/dataCount # as sample() takes an integer, we are creating a factor by which to get the approximate number of rows \n",
    "    print(frac) # just for checks :)\n",
    "    if (frac > 1): \n",
    "      frac = 1.0\n",
    "    print(frac) # just for checks++ :)\n",
    "    X = data.sample(True, frac) \n",
    "    print(X)\n",
    "    df = X.toPandas() # we then convert the Spark Dataframe to Pandas.  \n",
    "    print(\"everything worked\")\n",
    "  except Exception as e: \n",
    "    print(e)\n",
    "    print('use random data')\n",
    "    X = np.random.rand(nrows,ncols)\n",
    "    df = pd.DataFrame({'fea%d'%i:X[:,i] for i in range(X.shape[1])})\n",
    "    print(\"only random data\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Setting up data in Pandas Dataframe using Load data and null workaround function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nrows = 2**20\n",
    "nrows = int(nrows * 1.5)\n",
    "ncols = 400\n",
    "\n",
    "X = load_data(nrows,ncols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Intro to PCA parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look into all possible parameters that we can use when applying PCA: \n",
    "http://scikitlearn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "We will start here with the following :\n",
    "\n",
    "__n_components__ : int, float, None or string  \n",
    "Number of components to keep. if n_components is not set all components are kept\n",
    "\n",
    "__whiten__ : bool, optional (default False) \n",
    "When True (False by default) the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances. Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "\n",
    "__random_state__ : int, RandomState instance or None, optional (default None) \n",
    "If int, random_state is the seed used by the random number generator\n",
    "\n",
    "__svd_solver__ : string {‘auto’, ‘full’, ‘arpack’, ‘randomized’} \n",
    "If \"full\" :run exact full SVD calling the standard LAPACK solver via scipy.linalg.svd and select the components by postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "whiten = False\n",
    "random_state = 42\n",
    "svd_solver=\"full\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PCA on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the time needed to execute PCA function using standard sklearn library. \n",
    "__Note: this algorithm runs on CPU only.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca_sk = skPCA(n_components=n_components,svd_solver=svd_solver, \n",
    "            whiten=whiten, random_state=random_state)\n",
    "result_sk = pca_sk.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PCA on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we execute PCA function using RAPIDS cuml library we will first read the data in GPU data format using cudf. \n",
    "\n",
    "__cudf__ - GPU DataFrame manipulation library https://github.com/rapidsai/cudf\n",
    "\n",
    "__cuml__ - suite of libraries that implements a machine learning algorithms within the RAPIDS data science ecosystem https://github.com/rapidsai/cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = cudf.DataFrame.from_pandas(X) # Convert Pandas Dataframe to GPU Dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca_cuml = cumlPCA(n_components=n_components,svd_solver=svd_solver, \n",
    "            whiten=whiten, random_state=random_state)\n",
    "result_cuml = pca_cuml.fit_transform(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in ['singular_values_','components_','explained_variance_',\n",
    "             'explained_variance_ratio_']:\n",
    "    passed = array_equal(getattr(pca_sk,attr),getattr(pca_cuml,attr))\n",
    "    message = 'compare pca: cuml vs sklearn {:>25} {}'.format(attr,'equal' if passed else 'NOT equal')\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark ML accelerated with RAPIDS\n",
    "passed = array_equal(result_sk,result_cuml)\n",
    "message = 'compare pca: cuml vs sklearn transformed results %s'%('equal'if passed else 'NOT equal')\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "Databricks_RAPIDS_PCA_demo",
  "notebookId": 3941319872928235
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
