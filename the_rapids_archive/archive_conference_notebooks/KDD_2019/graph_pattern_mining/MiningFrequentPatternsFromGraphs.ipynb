{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enumerating BiCliques to Find Frequent Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KDD 2019 Workshop\n",
    "\n",
    "#### Authors\n",
    "- Tom Drabas (Microsoft)\n",
    "- Brad Rees (NVIDIA)\n",
    "- Juan-Arturo Herrera-Ortiz (Microsoft)\n",
    "\n",
    "#### Problem overview\n",
    "From time to time PCs running Microsoft Windows fail: a program might crash or hang, or you experience a kernel crash leading to the famous blue screen (we do love those 'Something went wrong' messages as well...;)).\n",
    "\n",
    "<img src=\"images/Windows_SomethingWentWrong.png\" alt=\"Windows problems\" width=380px class=\"center\"/>\n",
    "\n",
    "Well, when this happens it's not a good experience and we are truly interested in quickly finding out what might have gone wrong and/or at least what is common among the PCs that have failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy\n",
    "import azureml.core as aml\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The data prepared for this workshop will be available to download after the conference. We will share the link in the final notebook that will be available on RAPIDS github account. \n",
    "\n",
    "### Data\n",
    "The data we will be using in this workshop has been synthetically generate to showcase the type of scenarios we encounter in our work. \n",
    "\n",
    "While running certain workloads, PCs might fail for one reason or another. We collect the information from both types of scenarios and enrich the observations with the metadata about each PC (hardware, software, failure logs etc.). This forms a dataset where each row represents a PC and the features column contains a list of all the metadata we want to mine to find frequent patterns about the population that has failed. \n",
    "\n",
    "In this tutorial we will be representing this data in a form of a bi-partite graph. A bi-partite graph can be divided into two disconnected subgraphs (none of the vertices within the subgraphs are connected) with the edges connecting the vertices from one subgraph to the other. See the example below.\n",
    "\n",
    "<img src=\"images/BiPartiteGraph_Example.png\" alt=\"Bi-Partite graph example\" width=200px class=\"center\"/>\n",
    "\n",
    "In order to operate on this type of data we convert the list-of-features per row to a COO (Coordinate list) format: each row represents an edge connection, the first column contains the source vertex, the second one contains the destination vertex, and the third column contains the failure flag (0 = success, 1 = failure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1011392,0\n",
      "0,1014082,0\n",
      "0,1010117,0\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 ../../../../data/fpm_graph/coo_fpm.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data into a RAPIDS DataFrame `cudf`. ***NOTE: This will take longer than if you were running this on your local machine since the data-store is separate from this running VM. Normally it would be almost instant.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 980 ms, sys: 794 ms, total: 1.77 s\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fpm_df = cudf.read_csv('../../../../data/fpm_graph/coo_fpm.csv', names=['src', 'dst', 'flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.72 s, sys: 1.02 s, total: 9.75 s\n",
      "Wall time: 9.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fpm_pdf = pd.read_csv('../../../../data/fpm_graph/coo_fpm.csv', names=['src', 'dst', 'flag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data loaded let's check how big is our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row cnt: 41068808, col cnt: 3\n",
      "CPU times: user 499 µs, sys: 107 µs, total: 606 µs\n",
      "Wall time: 441 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "shp = fpm_df.shape\n",
    "print('Row cnt: {0}, col cnt: {1}'.format(*shp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have >41M records in our DataFrame. Let's see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    src      dst  flag\n",
      "0    0  1011392     0\n",
      "1    0  1014082     0\n",
      "2    0  1010117     0\n",
      "3    0  1010886     0\n",
      "4    0  1000005     0\n",
      "5    0  1005000     0\n",
      "6    0  1010251     0\n",
      "7    0  1003084     0\n",
      "8    0  1006540     0\n",
      "9    0  1012174     0\n"
     ]
    }
   ],
   "source": [
    "print(fpm_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the data\n",
    "\n",
    "Now that we have the data, let's explore it a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall failure rate\n",
    "First, let's find out what is the overall failure rate. In general, we do not want to extract any patterns that are below the overall failure rate since the would not help us to understand anything about the phenomenon we're dealing with nor would help us pinpoint the actual problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16653843471668328\n",
      "CPU times: user 0 ns, sys: 3.23 ms, total: 3.23 ms\n",
      "Wall time: 2.34 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(fpm_df['flag'].sum() / float(fpm_df['flag'].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the overall falure rate is 16.7%. However, you can see that running a `sum` and `count` reducers on 41M records took ~5-10ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device count\n",
    "\n",
    "I didn't tell you how many devices we included in the dataset. Let's figure it out. Since the `src` column contains multiple edges per PC we need to count only the unique ids for this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755775\n",
      "CPU times: user 518 ms, sys: 70 ms, total: 588 ms\n",
      "Wall time: 603 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(fpm_df['src'].unique().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 755k devices in the dataset and it took only 1s to find this out!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct features count\n",
    "Let's now check how many distinct meatadata features we included in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "CPU times: user 221 ms, sys: 119 ms, total: 340 ms\n",
      "Wall time: 348 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(fpm_df['dst'].unique().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see it is a synthetic dataset ;) We have a universe of 15k distinct metadata features each PC can be comprised of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different PCs have different number of features: some have two CPUs or 4 GPUs (lucky...). Below we can quickly find how many features each PCs has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    dst\n",
      "src\n",
      "0   70\n",
      "1   60\n",
      "2   74\n",
      "3   69\n",
      "4   77\n",
      "5   62\n",
      "6   62\n",
      "7   67\n",
      "8   53\n",
      "9   66\n",
      "[755765 more rows]\n",
      "CPU times: user 100 ms, sys: 39.1 ms, total: 139 ms\n",
      "Wall time: 157 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "degrees = fpm_df.groupby('src').agg({'dst': 'count'})\n",
    "print(degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average PCs have 54.34 components. The one with the max numer has 100.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'On average PCs have {0:.2f} components. The one with the max numer has {1}.'\n",
    "    .format(\n",
    "          degrees['dst'].mean()\n",
    "        , degrees['dst'].max()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the distribution of degrees\n",
    "\n",
    "We can very quickly calculate the deciles of degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Quantile   dst\n",
      "0       0.0  22.0\n",
      "1       0.1  38.0\n",
      "2       0.2  41.0\n",
      "3       0.3  44.0\n",
      "4       0.4  51.0\n",
      "5       0.5  56.0\n",
      "6       0.6  59.0\n",
      "7       0.7  62.0\n",
      "8       0.8  65.0\n",
      "9       0.9  70.0\n",
      "CPU times: user 91.4 ms, sys: 19.9 ms, total: 111 ms\n",
      "Wall time: 122 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "quantiles = degrees.quantile(q=[float(e) / 100 for e in range(0, 100, 10)])\n",
    "print(quantiles.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the distribution looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 229 ms, sys: 4.29 ms, total: 233 ms\n",
      "Wall time: 235 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "buckets = degrees['dst'].value_counts().reset_index().to_pandas()\n",
    "buckets.columns = ['Bucket', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 74 artists>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.bar(buckets['Bucket'], buckets['Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mine the data and find the bi-cliques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the tutorial we will show you the prototype implementation of the iMBEA algorithm proposed in Zhang, Y et al. paper from 2014 titled _On finding bicliques in bipartite graphs: A novel algorithm and its application to the integration of diverse biological data types_ published in BMC bioinformatics 15 (110) URL: https://www.researchgate.net/profile/Michael_Langston/publication/261732723_On_finding_bicliques_in_bipartite_graphs_A_novel_algorithm_and_its_application_to_the_integration_of_diverse_biological_data_types/links/00b7d53a300726c5b3000000/On-finding-bicliques-in-bipartite-graphs-A-novel-algorithm-and-its-application-to-the-integration-of-diverse-biological-data-types.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "First, we do some setting up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "# must be factor of 10\n",
    "PART_SIZE  = int(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data partitioning\n",
    "We partition the DataFrame into multiple parts to aid computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _partition_data_by_feature(_df) :\n",
    "    \n",
    "    #compute the number of sets\n",
    "    m = int(( _df['dst'].max() / PART_SIZE) + 1 )\n",
    "    \n",
    "    _ui = [None] * (m + 1)\n",
    "        \n",
    "    # Partition the data into a number of smaller DataFrame\n",
    "    s = 0\n",
    "    e = s + PART_SIZE\n",
    "\n",
    "    for i in range (m) :    \n",
    "        _ui[i] = _df.query('dst >= @s and dst < @e')\n",
    "\n",
    "        s = e\n",
    "        e = e + PART_SIZE   \n",
    "        \n",
    "    return _ui, m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enumerating features\n",
    "One of the key components of iMBEA algorithm is how it scans the graph starting from, in our case, from the most popular to the least popular feature. The `_count_features(...)` method below achieves exactly that and produces a sorted list of features ranked by their popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_features( _gdf, sort=True) :\n",
    "\n",
    "    aggs = OrderedDict()\n",
    "    aggs['dst'] = 'count'\n",
    "\n",
    "    c = fpm_df.groupby(['dst'], as_index=False).agg(aggs) \n",
    "    c = c.rename(columns={'dst':'count'})   \n",
    "    c = c.reset_index()\n",
    "\n",
    "    if (sort) :\n",
    "        c = c.sort_values(by='count', ascending=False)\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        dst  count\n",
      "1569  1001569  72525\n",
      "3045  1003045  72525\n",
      "7139  1007139  72525\n",
      "7665  1007665  72525\n",
      "9126  1009126  72525\n",
      "10095  1010095  72525\n",
      "12514  1012514  72525\n",
      "14378  1014378  72525\n",
      "2  1000002  62204\n",
      "1510  1001510  62204\n",
      "[14990 more rows]\n"
     ]
    }
   ],
   "source": [
    "print(_count_features(fpm_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental methods\n",
    "\n",
    "Below are some fundamental methods used iteratively by the final algorithm\n",
    "\n",
    "#### `get_src_from_dst`\n",
    "This method returns a DataFrame of all the source vertices that have the destination vertex `id` in their list of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all src vertices for a given dst\n",
    "def get_src_from_dst( _gdf, id) :\n",
    "    \n",
    "    _src_list = (_gdf.query('dst == @id'))\n",
    "    \n",
    "    _src_list.drop_column('dst')\n",
    "    \n",
    "    return _src_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `get_all_features`\n",
    "This method returns all the features that are connected to the vertices found in the `src_list_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the items used by the specified users\n",
    "def get_all_feature(_gdf, src_list_df, N) :\n",
    "    \n",
    "    c = [None] * N\n",
    "    \n",
    "    for i in range(N) :\n",
    "        c[i] = src_list_df.merge(_gdf[i], on='src', how=\"inner\")    \n",
    "  \n",
    "    return cudf.concat(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `is_same_as_last`\n",
    "This method checks if the bi-clicque has already been enumerated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_same_as_last(_old, _new) :\n",
    "    status = False\n",
    "    \n",
    "    if (len(_old) == len(_new)) :\n",
    "        m = _old.merge(_new, on='src', how=\"left\")  \n",
    "        \n",
    "        if m['src'].null_count == 0 :\n",
    "            status = True\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `update_results`\n",
    "This is a util method that helps to (1) maintain a DataFrame with enumerated bi-cliques that contains some of the `src` and `dst` vertices, and (2) some basic information about these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_results(m, f, key, b, s) :\n",
    "    \"\"\"\n",
    "    Input\n",
    "    * m = machines\n",
    "    * f = features\n",
    "    * key = cluster ID\n",
    "    * b = biclique answer\n",
    "    * s = stats answer \n",
    "\n",
    "    Returns\n",
    "    -------     \n",
    "    B : cudf.DataFrame      \n",
    "        A dataframe containing the list of machine and features.  This is not the full\n",
    "        edge list to save space.  Since it is a biclique, it is easy to recreate the edges  \n",
    "      \n",
    "        B['id']    - a cluster ID (this is a one up number - up to k)\n",
    "        B['vert']  - the vertex ID\n",
    "        B['type']  - 0 == machine, 1 == feature\n",
    "      \n",
    "    \n",
    "    S : cudf.DataFrame\n",
    "        A Pandas dataframe of statistics on the returned info.  \n",
    "        This dataframe is (relatively small) of size k.  \n",
    "        \n",
    "        S['id']       - the cluster ID\n",
    "        S['total']    - total vertex count\n",
    "        S['machines'] - number of machine nodes\n",
    "        S['features'] - number of feature vertices\n",
    "        S['bad_ratio'] - the ratio of bad machine / total machines\n",
    "    \"\"\"\n",
    "    \n",
    "    B = cudf.DataFrame()\n",
    "    S = cudf.DataFrame()\n",
    "\n",
    "    m_df = cudf.DataFrame()\n",
    "    m_df['vert'] = m['src'].astype(np.int32)\n",
    "    m_df['id']   = int(key)\n",
    "    m_df['type'] = int(0) \n",
    "    \n",
    "    \n",
    "    f_df = cudf.DataFrame()\n",
    "    f_df['vert'] = f['dst'].astype(np.int32)\n",
    "    f_df['id']   = int(key)\n",
    "    f_df['type'] = int(1)\n",
    "    \n",
    "    if len(b) == 0 :\n",
    "        B = cudf.concat([m_df, f_df])\n",
    "    else :\n",
    "        B = cudf.concat([b, m_df, f_df])\n",
    "    \n",
    "    # now update the stats\n",
    "    num_m = len(m_df)\n",
    "    num_f = len(f_df)\n",
    "    total = num_m# + num_f\n",
    "    \n",
    "    num_bad = len(m.query('flag == 1'))\n",
    "    ratio = num_bad / total\n",
    "\n",
    "    # now stats\n",
    "    s_tmp = cudf.DataFrame()\n",
    "    s_tmp['id']   = key\n",
    "    s_tmp['total'] = total \n",
    "    s_tmp['machines'] = num_m\n",
    "    s_tmp['bad_machines'] = num_bad\n",
    "    s_tmp['features'] = num_f\n",
    "    s_tmp['bad_ratio'] = ratio    \n",
    "    \n",
    "    if len(s) == 0 :\n",
    "        S = s_tmp\n",
    "    else :\n",
    "         S = cudf.concat([s,s_tmp])\n",
    "    \n",
    "    del m_df\n",
    "    del f_df\n",
    "    \n",
    "    return B, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ms_find_maximal_bicliques`\n",
    "This is the main loop for the algorithm. It iteratively scans the list of features and enumerates the bi-cliques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_find_maximal_bicliques(df, k, \n",
    "                              offset=0, \n",
    "                              max_iter=-1, \n",
    "                              support=1.0, \n",
    "                              min_features=1, \n",
    "                              min_machines=10) :\n",
    "    \"\"\"\n",
    "    Find the top k maximal bicliques\n",
    " \n",
    "    Parameters\n",
    "    ---------- \n",
    "    df :  cudf:DataFrame\n",
    "        A dataframe containing the bipartite graph edge list\n",
    "        Columns must be called 'src', 'dst', and 'flag'\n",
    "        \n",
    "    k  :  int\n",
    "        The max number of bicliques to return\n",
    "        -1 mean all\n",
    "    \n",
    "    offset : int\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------     \n",
    "    B : cudf.DataFrame      \n",
    "        A dataframe containing the list of machine and features.  This is not the full\n",
    "        edge list to save space.  Since it is a biclique, it is ease to recreate the edges  \n",
    "      \n",
    "        B['id']    - a cluster ID (this is a one up number - up to k)\n",
    "        B['vert']  - the vertex ID\n",
    "        B['type']  - 0 == machine, 1 == feature\n",
    "      \n",
    "    \n",
    "    S : cudf.DataFrame \n",
    "        A dataframe of statistics on the returned info.  \n",
    "        This dataframe is (relatively small) of size k.  \n",
    "        \n",
    "        S['id']       - the cluster ID\n",
    "        S['total']    - total vertex count\n",
    "        S['machines'] - number of machine nodes\n",
    "        S['features'] - number of feature vertices\n",
    "        S['bad_ration'] - the ratio of bad machine / total machines\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    x = [col for col in df.columns]\n",
    "    if 'src' not in x:\n",
    "       raise NameError('src column not found')\n",
    "    if 'dst' not in x:\n",
    "       raise NameError('dst column not found')   \n",
    "    if 'flag' not in x:\n",
    "       raise NameError('flag column not found')     \n",
    "\n",
    "    if support > 1.0 or support < 0.1:\n",
    "       raise NameError('support must be between 0.1 and 1.0')  \n",
    "    \n",
    "    \n",
    "    # this removes a prep step that offset the values for CUDA process\n",
    "    if offset > 0 :\n",
    "        df['dst'] = df['dst'] - offset    \n",
    "\n",
    "    # break the data into chunks to improve join/search performance \n",
    "    src_by_dst, num_parts = _partition_data_by_feature(df)    \n",
    "\n",
    "    # Get a list of all the dst (features) sorted by degree \n",
    "    f_list = _count_features(df, True)     \n",
    "\n",
    "    # create a dataframe for the answers\n",
    "    bicliques = cudf.DataFrame()\n",
    "    stats     = cudf.DataFrame()\n",
    "\n",
    "    # create a dataframe to help prevent duplication of work\n",
    "    machine_old = cudf.DataFrame()   \n",
    "    \n",
    "    # create a dataframe for stats\n",
    "    stats = cudf.DataFrame()\n",
    "    \n",
    "    answer_id = 0\n",
    "    iter_max = len(f_list)\n",
    "    \n",
    "    if max_iter != -1 :\n",
    "        iter_max = max_iter\n",
    "    \n",
    "    # Loop over all the features (dst) or until K is reached\n",
    "    for i in range(iter_max) :\n",
    "\n",
    "        # pop the next feature to process\n",
    "        feature = f_list['dst'][i]\n",
    "        degree  = f_list['count'][i]\n",
    "        \n",
    "        # compute the index to this item (which dataframe chunk is in)\n",
    "        idx = int(feature/PART_SIZE)        \n",
    "        \n",
    "        # get all machines that have this feature\n",
    "        machines = get_src_from_dst(src_by_dst[idx], feature) \n",
    "\n",
    "        # if this set of machines is the same as the last, skip this feature\n",
    "        if is_same_as_last(machine_old, machines) == False:\n",
    "\n",
    "            # now from those machines, hop out to the list of all the features \n",
    "            feature_list = get_all_feature(src_by_dst, machines, num_parts)   \n",
    "\n",
    "            # summarize occurrences\n",
    "            ic = _count_features(feature_list, True)\n",
    "\n",
    "            goal = int(degree * support)\n",
    "            \n",
    "            # only get dst nodes with the same degree\n",
    "            c = ic.query('count >= @goal')\n",
    "\n",
    "            # need more than X feature to make a biclique\n",
    "            if len(c) > min_features : \n",
    "                if len(machines) >= min_machines :\n",
    "                    bicliques, stats = update_results(machines, c, answer_id, bicliques, stats)\n",
    "\n",
    "                    answer_id = answer_id + 1\n",
    "            \n",
    "        # end - if same\n",
    "    \n",
    "        machine_old = machines\n",
    "    \n",
    "        if k > -1:\n",
    "            if answer_id == k :\n",
    "                break\n",
    "            \n",
    "    # end for loop\n",
    "\n",
    "    # All done, reset data\n",
    "    if offset > 0 :\n",
    "        df['dst'] = df['dst'] + offset  \n",
    "        \n",
    "    return bicliques, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding bi-cliques\n",
    "Now that we have a fundamental understanding how this works -- let's put it to action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 s, sys: 2.28 s, total: 8.28 s\n",
      "Wall time: 8.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bicliques, stats = ms_find_maximal_bicliques(\n",
    "    df=fpm_df, \n",
    "    k=10, \n",
    "    offset=1000000, \n",
    "    max_iter=100,\n",
    "    support = 1.0,\n",
    "    min_features=3,\n",
    "    min_machines=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes somewhere between <font size=\"10\">10 to 15 seconds</font> to analyze <font size=\"10\">>42M</font>edges and output the top 10 most important bicliques.\n",
    "\n",
    "Let's see what we got. We enumerated 10 bicliques. The worst of them had a failure rate of over 97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id  total  machines  bad_machines  features            bad_ratio\n",
      "0   0  72525     72525         52876         8   0.7290727335401586\n",
      "0   1  62204     62204         57025        19   0.9167416886373867\n",
      "0   2  43854     43854         42673        22    0.973069731381402\n",
      "0   3  41268     41268         17844        36  0.43239313753998254\n",
      "0   4  37745     37745          3085        41   0.0817326798251424\n",
      "0   5  30751     30751         12868        54   0.4184579363272739\n",
      "0   6  14713     14713         14298        57   0.9717936518724937\n",
      "0   7  14369     14369         13970        59   0.9722318880924212\n",
      "0   8  11282     11282         10009        66   0.8871653962063464\n",
      "0   9   6289      6289          6133        70   0.9751947845444426\n"
     ]
    }
   ],
   "source": [
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the one of the worst ones that affected the most machines: over 57k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2            2\n",
       "1510      1510\n",
       "1569      1569\n",
       "1604      1604\n",
       "2964      2964\n",
       "3045      3045\n",
       "3581      3581\n",
       "5390      5390\n",
       "7139      7139\n",
       "7665      7665\n",
       "7686      7686\n",
       "8226      8226\n",
       "9087      9087\n",
       "9126      9126\n",
       "10095    10095\n",
       "12514    12514\n",
       "14376    14376\n",
       "14378    14378\n",
       "14579    14579\n",
       "Name: vert, dtype: int32"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bicliques.query('id == 1 and type == 1')['vert'].sort_values().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you change the `type` to `0` we could retrieve a sample list of PCs that fit this particular pattern/bi-clique: this is useful and sometimes helps us to further narrow down a problem by further scanning the logs from PCs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
