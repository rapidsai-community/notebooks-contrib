{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.1 - Feature Engineering for XGBoost Model\n",
    "In this notebook, we will engineer aggregated features based on the statistical properties of the flux and observation times.\n",
    "\n",
    "This notebook is broken down into five sections\n",
    "1. Compute the flux skewness for each timeseries sequence\n",
    "2. Compute other aggregated features from `flux` and observation time\n",
    "3. Load training labels and merge with training set\n",
    "4. Concatenate features from Parts 1 and 2\n",
    "5. Concatenate bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf as gd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from numba import cuda,jit,float32\n",
    "\n",
    "from utils import scatter, groupby_skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set()\n",
    "print(gd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../../../../../data/plasticc_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_cols = ['object_id', 'mjd', 'passband', 'flux', 'flux_err', 'detected']\n",
    "ts_dtypes = ['int32', 'float32', 'int32', 'float32','float32','int32']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 - Compute Flux Skewness for each timeseries sequence\n",
    "\n",
    "Load the training and test datasets back into cuDF DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gd = gd.read_csv('%s/training_set.csv'%PATH, names=ts_cols,dtype=ts_dtypes,skiprows=1)\n",
    "test_gd = gd.read_csv('%s/test_set_sample.csv'%PATH, names=ts_cols,dtype=ts_dtypes,skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gd.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, oid in zip(range(1), train_gd.object_id.unique()[:1]):\n",
    "    train = train_gd.to_pandas()\n",
    "    mask = train.object_id== oid\n",
    "\n",
    "    scatter(train.loc[mask,'mjd'].values,\n",
    "            train.loc[mask,'flux'].values,\n",
    "            values=train.loc[mask,'passband'].values,\n",
    "            xlabel='time',\n",
    "            ylabel='flux',\n",
    "            title='Object %d class 42'%oid)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to be computing the skewness of the `flux` for each `object_id`, we can safely drop the other columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_flux_skew_gd = test_gd[['object_id','flux']]\n",
    "train_flux_skew_gd = train_gd[['object_id','flux']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation of the flux skewness is done using the `groupby() - apply_grouped()` techinque we used in the RNN feature engineering stage.\n",
    "\n",
    "We will use the `groupby_skew()` helper function, which can be found in the supplementary Python script `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_flux_skew_gd = groupby_skew(test_flux_skew_gd, \"object_id\", \"flux\")\n",
    "train_flux_skew_gd = groupby_skew(train_flux_skew_gd, \"object_id\", \"flux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flux_skew_gd.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Compute Statistical Summary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While very useful, the flux skew, alone, will probably not train a classifier with good accuracy. In this part, we will engineer more features by aggregating existing features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Exercise\n",
    "\n",
    "Wsing the functions `groupby()` and `agg()`, cuDF can build aggregations for many pre-defined functions, such as `max`, `min`, and `mean`. \n",
    "\n",
    "Play around with this strategy in the cell that follows, to get familiar with how this behavior works. When you feel you have a good understanding, fill in the `perform_aggregation()` function so that the remaining aggregated features can be computed in `compute_aggregated_features()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gd.groupby(, as_index=False).agg().head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_aggregation(df, groupby_col, agg_col, agg_type):\n",
    "    return # Fill in the groupby().agg() to enable the remaining feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_aggs(df,aggs,col = \"object_id\"):\n",
    "    res = None\n",
    "    for i,j in aggs.items():\n",
    "        for k in j:\n",
    "            tmp = perform_aggregation(df, col, i, k)\n",
    "            tmp.columns = [col,'%s_%s'%(k,i)]\n",
    "            res = tmp if res is None else res.merge(tmp,on=[col],how='left')\n",
    "        df.drop_column(i)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aggregated_features(df):\n",
    "    \"\"\"\n",
    "    Engineer new features by aggregating existing features\n",
    "    \"\"\"\n",
    "    \n",
    "    aggs = {\n",
    "        'passband': ['mean'],  # mean passband\n",
    "        'detected': ['mean'],  # mean detected\n",
    "        'mjd':['max','min'],   # min / max time range\n",
    "    }\n",
    "    \n",
    "    agg_df = groupby_aggs(df, aggs)\n",
    "    \n",
    "    # If flux uncertanty is low, we get a high ratio squared. If the uncertainty is high, \n",
    "    # we will get a low ratio squared.\n",
    "    df['flux_ratio_sq'] = df['flux'] / df['flux_err']\n",
    "    df['flux_ratio_sq'] = df['flux_ratio_sq'].applymap(lambda x: math.pow(x,2))\n",
    "    \n",
    "    # Multiply flux by the ratio squared\n",
    "    df['flux_by_flux_ratio_sq'] = df['flux'] * df['flux_ratio_sq']\n",
    "    \n",
    "    aggs2 = {\n",
    "        'flux_ratio_sq':['sum'],            # Sum the sq flux ratios\n",
    "        'flux_by_flux_ratio_sq':['sum'],    # Sum the flux * sq flux ratios\n",
    "        'flux': ['min', 'max', 'mean'],     # Summary stats for flux\n",
    "        'flux_err': ['min', 'max', 'mean'], # Summary stats for flux certainty\n",
    "    }\n",
    "    \n",
    "    agg_df2 = groupby_aggs(df, aggs2)\n",
    "    agg_df = agg_df.merge(agg_df2,on=['object_id'],how='left')\n",
    "    del agg_df2\n",
    "\n",
    "    agg_df['flux_diff'] = agg_df['max_flux'] - agg_df['min_flux']\n",
    "    agg_df['flux_dif2'] = (agg_df['max_flux'] - agg_df['min_flux']) / agg_df['mean_flux']\n",
    "    \n",
    "    agg_df['flux_w_mean'] = agg_df['sum_flux_by_flux_ratio_sq'] / agg_df['sum_flux_ratio_sq']\n",
    "    agg_df['flux_dif3'] = (agg_df['max_flux'] - agg_df['min_flux']) / agg_df['flux_w_mean']\n",
    "    \n",
    "    agg_df['mjd_diff'] = agg_df['max_mjd'] - agg_df['min_mjd']\n",
    "    agg_df.drop_column('max_mjd')\n",
    "    agg_df.drop_column('min_mjd')\n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_gd = compute_aggregated_features(train_gd)\n",
    "test_final_gd = compute_aggregated_features(test_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_gd.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Load Training Labels and Metadata\n",
    "\n",
    "Metadata is supplied in a separate CSV file for each `object_id` in our training set. This metadata also includes a `target` column, which are the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['object_id', 'ra', 'decl', 'gal_l', 'gal_b', 'ddf',\n",
    "       'hostgal_specz', 'hostgal_photoz', 'hostgal_photoz_err', \n",
    "       'distmod','mwebv', 'target']\n",
    "\n",
    "dtypes = ['int32']+['float32']*4+['int32']+['float32']*5+['int32']\n",
    "\n",
    "train_meta_gd = gd.read_csv('%s/training_set_metadata.csv'%PATH, names=cols, dtype=dtypes, skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop columns we won't need for training our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['ra','decl','gal_l','gal_b']:\n",
    "    train_meta_gd.drop_column(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the metadata with our training set by `object_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_gd = train_meta_gd.merge(train_final_gd,on=['object_id'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now safely delete some of the dataframes we no longer need in order to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_gd, train_meta_gd, test_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Merge Flux Skew & Statistical Summaries\n",
    "\n",
    "We use cuDF's `merge()` to combine the flux skew and statistical summary Dataframes by `object_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_gd = train_final_gd.merge(train_flux_skew_gd,on=['object_id'],how='left')\n",
    "test_final_gd = test_final_gd.merge(test_flux_skew_gd,on=['object_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_gd.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 - Merge in Bottleneck Features\n",
    "\n",
    "Load the bottleneck features extracted from the RNN in `Part 1.2` and concatenate with the aggregated features we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bn = gd.from_pandas(pd.read_pickle(\"train_bn.pkl\"))\n",
    "test_bn = gd.from_pandas(pd.read_pickle(\"test_bn.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_gd = train_final_gd.merge(train_bn,on=['object_id'],how='left')\n",
    "test_final_gd = test_final_gd.merge(test_bn,on=['object_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_gd.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_bn,test_bn  # Save device? memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Final Train/Test Data to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store our extracted data out to csv files so that we can use it downstream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_gd.to_pandas().to_pickle(\"train_gdf.pkl\")\n",
    "test_final_gd.to_pandas().to_pickle(\"test_gdf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
