{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDD 2020 \n",
    "# Where really are the parking spots?\n",
    "Using RAPIDS to find proper distances to parking spots in Seattle.\n",
    "\n",
    "## Load the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuspatial import haversine_distance\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import cugraph\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "dtypes = OrderedDict([\n",
    "    ('OccupancyDateTime', 'date'),\n",
    "    ('PaidOccupancy', 'int64'),\n",
    "    ('BlockfaceName', 'str'),\n",
    "    ('SideOfStreet', 'str'),\n",
    "    ('SourceElementKey', 'int64'),\n",
    "    ('ParkingTimeLimitCategory', 'int64'),\n",
    "    ('ParkingSpaceCount', 'int64'),\n",
    "    ('PaidParkingArea', 'str'),\n",
    "    ('PaidParkingSubArea', 'str'),\n",
    "    ('PaidParkingRate', 'int8'),\n",
    "    ('ParkingCategory', 'str'),\n",
    "    ('Location', 'str'),\n",
    "    ('dow', 'int8')\n",
    "])\n",
    "\n",
    "df = cudf.read_csv(\n",
    "    '../data/parking_MayJun2019.csv'\n",
    "    , skiprows=1\n",
    "    , dtype=list(dtypes.values())\n",
    "    , names=list(dtypes.keys())\n",
    ")\n",
    "\n",
    "df = df[['SourceElementKey', 'Location']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the geo-coordinates for the parking locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 611 Âµs\n"
     ]
    }
   ],
   "source": [
    "def extractLon(location):\n",
    "    lon = location.str.extract('([0-9\\.\\-]+) ([0-9\\.]+)')[0]\n",
    "    return lon\n",
    "\n",
    "def extractLat(location):\n",
    "    lon = location.str.extract('([0-9\\.\\-]+) ([0-9\\.]+)')[1]\n",
    "    return lon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "locations = df.drop_duplicates()\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.96 ms\n"
     ]
    }
   ],
   "source": [
    "locations['longitude'] = extractLon(locations['Location']).astype('float')\n",
    "locations['latitude'] = extractLat(locations['Location']).astype('float')\n",
    "locations = locations[['SourceElementKey', 'longitude', 'latitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll use the Nomatim encoder to find the coordinates of the Space Needle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location(Space Needle, 400, Broad Street, South Lake Union, Belltown, Seattle, King County, Washington, 98109, United States of America, (47.6205131, -122.34930359883187, 0.0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 629 ms\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"todrabas_test\")\n",
    "location = geolocator.geocode(\"400 Broad St, Seattle, WA 98109\") # SPACE NEEDLE\n",
    "\n",
    "location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As crow flies vs as people walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the graph data\n",
    "Thanks to John Murray for analyzing the map of King County roads and producing the data we will now use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and unpack the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.23 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory  = os.path.exists('../data')\n",
    "archive    = os.path.exists('../data/king_county_road_graph_20190909.tar.gz')\n",
    "file_graph = os.path.exists('../data/king_county_road_graph_20190909.csv')\n",
    "file_nodes = os.path.exists('../data/king_county_road_nodes_20190909.csv')\n",
    "\n",
    "if not directory:\n",
    "    os.mkdir('../data')\n",
    "\n",
    "if not archive:\n",
    "    import wget, shutil\n",
    "    \n",
    "    wget.download('http://tomdrabas.com/data/seattle_parking/king_county_road_graph_20190909.tar.gz')\n",
    "    shutil.move('king_county_road_graph_20190909.tar.gz', '../data/king_county_road_graph_20190909.tar.gz')\n",
    "    \n",
    "if not file_graph or not file_nodes:\n",
    "    import tarfile\n",
    "\n",
    "    tf = tarfile.open('../data/king_county_road_graph_20190909.tar.gz')\n",
    "    tf.extractall(path='../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's read the King County road data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.2 ms\n"
     ]
    }
   ],
   "source": [
    "road_graph_data = cudf.read_csv('../data/king_county_road_graph_20190909.csv')\n",
    "road_graph_data['node1'] = road_graph_data['node1'].astype('int32')\n",
    "road_graph_data['node2'] = road_graph_data['node2'].astype('int32')\n",
    "road_graph_data['LENGTH'] = road_graph_data['LENGTH'] * 3 # convert to feet as the LENGHT was given in yards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.73 ms\n"
     ]
    }
   ],
   "source": [
    "road_nodes = cudf.read_csv('../data/king_county_road_nodes_20190909.csv')\n",
    "road_nodes['NodeID'] = road_nodes['NodeID'].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the maximum of the `NodeID` so we can later append the additional nodes that will be perpendicular to the actual parking locations. We also specify the offset - this will be used to append parking nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127380"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "offset = 100000\n",
    "nodeId = road_nodes['NodeID'].max()                       ## so we can number the parking nodes properly (since we'll be adding a perpendicular projections)\n",
    "parking_nodes_idx = road_nodes['NodeID'].max() + offset   ## retain it so we can later filter the results to only parking locations\n",
    "nodeId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move all the parking locations to host (via `.to_pandas()` method) so we can loop through all the ~1500 parking locations. Here, we also create an empty DataFrame that will hold the parking location nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.9 ms\n"
     ]
    }
   ],
   "source": [
    "parking_locations = locations.to_pandas().to_dict('records')\n",
    "parking_locations_nodes = cudf.DataFrame(columns=['NodeID', 'Lon', 'Lat', 'SourceElementKey'])\n",
    "added_location_edges    = cudf.DataFrame(columns=['node1', 'node2', 'LENGTH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the parking data. The kernel below finds equations of two lines:\n",
    "\n",
    "1. Line that goes through road intersections\n",
    "2. Line that is perpendicular to (1) and goes through the parking location.\n",
    "\n",
    "Ultimately, we are finind the intersection of these two lines -- we call it the `PROJ` point below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.34 ms\n"
     ]
    }
   ],
   "source": [
    "def kernel_find_projection(Lon_x, Lat_x, Lon_y, Lat_y, Lon_PROJ, Lat_PROJ, Lon_REF, Lat_REF):\n",
    "    for i, (lon_x, lat_x, lon_y, lat_y) in enumerate(zip(Lon_x, Lat_x, Lon_y, Lat_y)):\n",
    "        # special case where A and B have the same LON i.e. vertical line\n",
    "        if lon_x == lon_y:\n",
    "            Lon_PROJ[i] = lon_x\n",
    "            Lat_PROJ[i] = Lat_REF    \n",
    "        else:\n",
    "            # find slope\n",
    "            a_xy = (lat_x - lat_y) / float(lon_x - lon_y)\n",
    "\n",
    "            # special case where A and B have the same LAT i.e. horizontal line\n",
    "            if a_xy == 0:\n",
    "                Lon_PROJ[i] = Lon_REF\n",
    "                Lat_PROJ[i] = lat_x\n",
    "            else: \n",
    "                # if neither of the above special cases apply\n",
    "                # find the equation of the perpendicular line\n",
    "                a_R  = -1 / a_xy                    ### SLOPE\n",
    "\n",
    "                # find intersections\n",
    "                b_xy = lat_x - a_xy * lon_x\n",
    "                b_R  = Lat_REF - a_R  * Lon_REF\n",
    "\n",
    "                # find the coordinates\n",
    "                Lon_PROJ[i] = (b_R - b_xy) / (a_xy - a_R)\n",
    "                Lat_PROJ[i] = a_R * Lon_PROJ[i] + b_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parking locations: 1,473\n",
      "Processed: 0 (0.000000%) nodes\n",
      "Processed: 100 (6.788866%) nodes\n",
      "Processed: 200 (13.577733%) nodes\n",
      "Processed: 300 (20.366599%) nodes\n",
      "Processed: 400 (27.155465%) nodes\n",
      "Processed: 500 (33.944331%) nodes\n",
      "Processed: 600 (40.733198%) nodes\n",
      "Processed: 700 (47.522064%) nodes\n",
      "Processed: 800 (54.310930%) nodes\n",
      "Processed: 900 (61.099796%) nodes\n",
      "Processed: 1,000 (67.888663%) nodes\n",
      "Processed: 1,100 (74.677529%) nodes\n",
      "Processed: 1,200 (81.466395%) nodes\n",
      "Processed: 1,300 (88.255261%) nodes\n",
      "Processed: 1,400 (95.044128%) nodes\n",
      "Finished processing...\n",
      "time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "parking_locations_cnt = len(parking_locations)\n",
    "print('Number of parking locations: {0:,}'.format(parking_locations_cnt))\n",
    "\n",
    "for i, loc in enumerate(parking_locations):\n",
    "    if i % 100 == 0:\n",
    "        print('Processed: {0:,} ({1:2%}) nodes'.format(i, i/float(parking_locations_cnt)))\n",
    "        \n",
    "    #### INCREASE THE COUNTER AND GET THE REFERENCE POINT\n",
    "    nodeId = nodeId + 1\n",
    "    lat_r = loc['latitude']\n",
    "    lon_r = loc['longitude']\n",
    "\n",
    "    #### APPEND GEO COORDINATES TO INTERSECTION AND SUBSET DOWN THE DATASET\n",
    "    #### TO POINTS WITHIN ~2000ft FROM PARKING SPOT\n",
    "    paths = (\n",
    "        road_graph_data\n",
    "        .rename(columns={'node1': 'NodeID'})\n",
    "        .merge(road_nodes[['NodeID', 'Lat', 'Lon']], on='NodeID', how='left')\n",
    "        .rename(columns={'NodeID': 'node1', 'node2': 'NodeID'})\n",
    "        .merge(road_nodes[['NodeID', 'Lat', 'Lon']], on='NodeID', how='left')\n",
    "        .rename(columns={'NodeID': 'node2'})\n",
    "        .query('Lat_x >= (@lat_r - 0.0075) and Lat_x <= (@lat_r + 0.0075)')\n",
    "        .query('Lon_x >= (@lon_r - 0.0075) and Lon_x <= (@lon_r + 0.0075)')\n",
    "        .query('Lat_y >= (@lat_r - 0.0075) and Lat_y <= (@lat_r + 0.0075)')\n",
    "        .query('Lon_y >= (@lon_r - 0.0075) and Lon_y <= (@lon_r + 0.0075)')\n",
    "    )\n",
    "\n",
    "    #### APPEND THE PARKING LOCATION SO WE CAN CALCULATE DISTANCES\n",
    "    paths['Lon_REF'] = loc['longitude']\n",
    "    paths['Lat_REF'] = loc['latitude']\n",
    "\n",
    "    paths = paths.apply_rows(\n",
    "        kernel_find_projection\n",
    "        , incols  = ['Lon_x', 'Lat_x', 'Lon_y', 'Lat_y', 'Lon_REF', 'Lat_REF']\n",
    "        , outcols = {'Lon_PROJ': np.float64, 'Lat_PROJ': np.float64}\n",
    "        , kwargs  = {'Lon_REF': loc['longitude'], 'Lat_REF': loc['latitude']}\n",
    "    )\n",
    "\n",
    "    #### CALCULATE THE DISTANCES SO WE CAN CHECK IF THE PROJ POINT IS BETWEEN ROAD NODES\n",
    "    paths['Length_x_PROJ'] = haversine_distance(\n",
    "              paths['Lon_x']\n",
    "            , paths['Lat_x']\n",
    "            , paths['Lon_PROJ']\n",
    "            , paths['Lat_PROJ'])\n",
    "\n",
    "    paths['Length_y_PROJ'] = haversine_distance(\n",
    "              paths['Lon_y']\n",
    "            , paths['Lat_y']\n",
    "            , paths['Lon_PROJ']\n",
    "            , paths['Lat_PROJ'])\n",
    "\n",
    "    paths['Length_REF_PROJ'] = haversine_distance(\n",
    "              paths['Lon_REF']\n",
    "            , paths['Lat_REF']\n",
    "            , paths['Lon_PROJ']\n",
    "            , paths['Lat_PROJ'])\n",
    "    \n",
    "    paths['Length_x_PROJ']   = paths['Length_x_PROJ'] * 0.621371 * 5280\n",
    "    paths['Length_y_PROJ']   = paths['Length_y_PROJ'] * 0.621371 * 5280\n",
    "    paths['Length_REF_PROJ'] = paths['Length_REF_PROJ'] * 0.621371 * 5280\n",
    "\n",
    "    #### SELECT THE POINTS THAT A LESS THAN OR EQAL TO TOTAL LENGTH OF THE EDGE (WITHIN 1 ft)\n",
    "    paths['PROJ_between'] = (paths['Length_x_PROJ'] + paths['Length_y_PROJ']) <= (paths['LENGTH'] + 4)\n",
    "    \n",
    "    #### SELECT THE CLOSEST\n",
    "    closest = (\n",
    "        paths\n",
    "        .query('PROJ_between')\n",
    "        .nsmallest(1, 'Length_REF_PROJ')\n",
    "        .to_pandas()\n",
    "        .to_dict('records')[0]\n",
    "    )\n",
    "    \n",
    "    # add nodes\n",
    "    nodes =    cudf.DataFrame({\n",
    "          'NodeID': [nodeId + offset, nodeId]\n",
    "        , 'Lon':    [closest['Lon_REF'], closest['Lon_PROJ']]\n",
    "        , 'Lat':    [closest['Lat_REF'], closest['Lat_PROJ']]\n",
    "        , 'SourceElementKey': [loc['SourceElementKey'], None]\n",
    "    })\n",
    "\n",
    "    parking_locations_nodes = cudf.concat([parking_locations_nodes, nodes])\n",
    "\n",
    "    # add edges (bi-directional)\n",
    "    edges = cudf.DataFrame({\n",
    "          'node1':  [nodeId, nodeId, nodeId, closest['node1'], closest['node2'], nodeId + offset]\n",
    "        , 'node2':  [closest['node1'], closest['node2'], nodeId + offset, nodeId, nodeId, nodeId]\n",
    "        , 'LENGTH': [\n",
    "              closest['Length_x_PROJ'], closest['Length_y_PROJ'], closest['Length_REF_PROJ']\n",
    "            , closest['Length_x_PROJ'], closest['Length_y_PROJ'], closest['Length_REF_PROJ']\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    added_location_edges = cudf.concat([added_location_edges, edges]) ## append to the temp DataFrame\n",
    "\n",
    "print('Finished processing...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.1 ms\n"
     ]
    }
   ],
   "source": [
    "road_nodes = (\n",
    "    cudf\n",
    "    .concat([road_nodes[['NodeID', 'Lon', 'Lat']], parking_locations_nodes])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can find the nearest intersections from the Space Needle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node1</th>\n",
       "      <th>node2</th>\n",
       "      <th>LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47756</th>\n",
       "      <td>128855</td>\n",
       "      <td>47757</td>\n",
       "      <td>175.906391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80448</th>\n",
       "      <td>128855</td>\n",
       "      <td>80449</td>\n",
       "      <td>200.062128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96739</th>\n",
       "      <td>128855</td>\n",
       "      <td>96740</td>\n",
       "      <td>261.056715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108797</th>\n",
       "      <td>128855</td>\n",
       "      <td>108798</td>\n",
       "      <td>277.221141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47827</th>\n",
       "      <td>128855</td>\n",
       "      <td>47828</td>\n",
       "      <td>301.715490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         node1   node2      LENGTH\n",
       "47756   128855   47757  175.906391\n",
       "80448   128855   80449  200.062128\n",
       "96739   128855   96740  261.056715\n",
       "108797  128855  108798  277.221141\n",
       "47827   128855   47828  301.715490"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 287 ms\n"
     ]
    }
   ],
   "source": [
    "road_nodes['Lon_REF'] = location.longitude\n",
    "road_nodes['Lat_REF'] = location.latitude\n",
    "\n",
    "road_nodes['Distance'] = haversine_distance(\n",
    "          road_nodes['Lon']\n",
    "        , road_nodes['Lat']\n",
    "        , road_nodes['Lon_REF']\n",
    "        , road_nodes['Lat_REF'])\n",
    "road_nodes['Distance'] = road_nodes['Distance'] * 0.621371 * 5280\n",
    "\n",
    "space_needle_to_nearest_intersection = road_nodes.nsmallest(5, 'Distance') ### Space Needle is surrounded by around 5 road intersections hence we add 5\n",
    "space_needle_to_nearest_intersection_dist = space_needle_to_nearest_intersection['Distance'].to_array()[0]\n",
    "\n",
    "space_needle_to_nearest_intersection['node1'] = nodeId + 2\n",
    "space_needle_to_nearest_intersection = (\n",
    "    space_needle_to_nearest_intersection\n",
    "    .rename(columns={'NodeID': 'node2', 'Distance': 'LENGTH'})\n",
    "    [['node1', 'node2', 'LENGTH']]\n",
    ")\n",
    "\n",
    "road_graph_data = cudf.concat([space_needle_to_nearest_intersection, added_location_edges, road_graph_data])\n",
    "space_needle_to_nearest_intersection ### SHOW THE EDGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The road graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 56.8 ms\n"
     ]
    }
   ],
   "source": [
    "road_graph_data = road_graph_data.reset_index(drop=True)\n",
    "road_graph_data['node1'] = road_graph_data['node1'].astype('int32')\n",
    "road_graph_data['node2'] = road_graph_data['node2'].astype('int32')\n",
    "\n",
    "g = cugraph.Graph()\n",
    "g.from_cudf_edgelist(road_graph_data, source='node1', destination='node2', edge_attr='LENGTH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `.sssp(...)` method from `cugraph` to find the shortest distances to parking spots from the Space Needle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>vertex</th>\n",
       "      <th>predecessor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93958</th>\n",
       "      <td>978.340665</td>\n",
       "      <td>227822</td>\n",
       "      <td>127822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93959</th>\n",
       "      <td>954.882271</td>\n",
       "      <td>227823</td>\n",
       "      <td>127823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103850</th>\n",
       "      <td>978.714937</td>\n",
       "      <td>227666</td>\n",
       "      <td>127666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103851</th>\n",
       "      <td>986.632352</td>\n",
       "      <td>227667</td>\n",
       "      <td>127667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105152</th>\n",
       "      <td>796.521771</td>\n",
       "      <td>227912</td>\n",
       "      <td>127912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105153</th>\n",
       "      <td>793.130113</td>\n",
       "      <td>227913</td>\n",
       "      <td>127913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109108</th>\n",
       "      <td>494.541097</td>\n",
       "      <td>227585</td>\n",
       "      <td>127585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109109</th>\n",
       "      <td>471.843338</td>\n",
       "      <td>227586</td>\n",
       "      <td>127586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111643</th>\n",
       "      <td>783.243667</td>\n",
       "      <td>228506</td>\n",
       "      <td>128506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128846</th>\n",
       "      <td>982.444003</td>\n",
       "      <td>228224</td>\n",
       "      <td>128224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129551</th>\n",
       "      <td>587.108025</td>\n",
       "      <td>228160</td>\n",
       "      <td>128160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129552</th>\n",
       "      <td>542.478604</td>\n",
       "      <td>228161</td>\n",
       "      <td>128161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          distance  vertex  predecessor\n",
       "93958   978.340665  227822       127822\n",
       "93959   954.882271  227823       127823\n",
       "103850  978.714937  227666       127666\n",
       "103851  986.632352  227667       127667\n",
       "105152  796.521771  227912       127912\n",
       "105153  793.130113  227913       127913\n",
       "109108  494.541097  227585       127585\n",
       "109109  471.843338  227586       127586\n",
       "111643  783.243667  228506       128506\n",
       "128846  982.444003  228224       128224\n",
       "129551  587.108025  228160       128160\n",
       "129552  542.478604  228161       128161"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 320 ms\n"
     ]
    }
   ],
   "source": [
    "all_distances = cugraph.sssp(g, nodeId + 2)\n",
    "distances = all_distances.query('vertex > @parking_nodes_idx and distance < 1000')\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cugraph` returns a DataFrame with vertex, distance to that vertex, and the total distance traveled to that vertex from the `nodeId + 1` node -- the Space Needle. Here, we unfold the full path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing record: 0\n",
      "Processing record: 1\n",
      "Processing record: 2\n",
      "Processing record: 3\n",
      "Processing record: 4\n",
      "Processing record: 5\n",
      "Processing record: 6\n",
      "Processing record: 7\n",
      "Processing record: 8\n",
      "Processing record: 9\n",
      "Processing record: 10\n",
      "Processing record: 11\n",
      "time: 622 ms\n"
     ]
    }
   ],
   "source": [
    "# unfold -- create the whole path\n",
    "closest_node = nodeId + 2\n",
    "parking_cnt = distances['vertex'].count()\n",
    "\n",
    "for i in range(parking_cnt):\n",
    "    print('Processing record: {0}'.format(i))\n",
    "    parking_node = distances.iloc[i].to_pandas()\n",
    "    vertex = int(parking_node[1])\n",
    "    predecessor = int(parking_node[2])\n",
    "    \n",
    "    if i == 0:\n",
    "        paths = all_distances.query('vertex == @vertex')\n",
    "    else:\n",
    "        paths = cudf.concat([all_distances.query('vertex == @vertex'), paths])\n",
    "\n",
    "    while vertex != closest_node:\n",
    "        temp = all_distances.query('vertex == @predecessor')\n",
    "        paths = cudf.concat([temp, paths])\n",
    "        predecessor = temp['predecessor'].to_array()[0]\n",
    "        vertex = temp['vertex'].to_array()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charting the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 126 ms\n"
     ]
    }
   ],
   "source": [
    "paths['vertex'] = paths['vertex'].astype('int64')\n",
    "paths['predecessor'] = paths['predecessor'].astype('int64')\n",
    "paths = paths.drop_duplicates()\n",
    "\n",
    "### process the data so we get the Lat/Lon back for both src and dest\n",
    "### then move to host\n",
    "paths_host = (\n",
    "    paths\n",
    "    .rename(columns={'vertex': 'NodeID'})\n",
    "    .merge(road_nodes[['NodeID', 'Lat', 'Lon']], on='NodeID', how='left')\n",
    "    .rename(columns={'NodeID': 'vertex', 'predecessor': 'NodeID'})\n",
    "    .merge(road_nodes[['NodeID', 'Lat', 'Lon']], on='NodeID', how='left')\n",
    "    .fillna({'Lat_y': location.latitude, 'Lon_y': location.longitude})\n",
    "    [['vertex', 'Lat_x', 'Lon_x', 'Lat_y', 'Lon_y']]\n",
    "    .query('vertex != @nodeId + 2')\n",
    "    .to_pandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the information about the parking spots so we can create info boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.53 ms\n"
     ]
    }
   ],
   "source": [
    "distances['vertex'] = distances['vertex'].astype('int64')\n",
    "distances_host = (\n",
    "    distances\n",
    "    .rename(columns={'vertex': 'NodeID'})\n",
    "    .merge(road_nodes[['NodeID', 'Lat', 'Lon', 'SourceElementKey']], on='NodeID')\n",
    "    [['SourceElementKey', 'Lat', 'Lon', 'distance']]\n",
    "    .to_pandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 962 Âµs\n"
     ]
    }
   ],
   "source": [
    "info_box_template = \"\"\"\n",
    "<dl>\n",
    "<dt><dd>SourceElementKey</dd><dd>{SourceElementKey}</dd></dt>\n",
    "<dt><dd>Distance        </dd><dd>{distance:.0f} ft.</dd></dt>\n",
    "</dl>\n",
    "\"\"\"\n",
    "\n",
    "parking_info = [info_box_template.format(**parking) for parking in distances_host.to_dict('records')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And... plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "import gmaps\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "####################################################\n",
    "##                                                ##\n",
    "## CHANGE THE API CREDS IN THE GoogleMapsAPI.cred ##\n",
    "##                                                ##\n",
    "####################################################\n",
    "with open('config/GoogleMapsAPI.cred', 'r') as f:\n",
    "    cred = f.read()\n",
    "    \n",
    "gmaps.configure(api_key=cred) # Your Google API key, go to https://console.developers.google.com\n",
    "\n",
    "parking_layer = gmaps.symbol_layer(\n",
    "    distances_host[['Lat', 'Lon']], fill_color=\"green\", stroke_color=\"green\", scale=3, info_box_content=parking_info\n",
    ")\n",
    "\n",
    "destinations_layer = gmaps.symbol_layer(\n",
    "    [[location.latitude, location.longitude]]\n",
    "    , info_box_content=['DESTINATION']\n",
    "    , scale=5\n",
    "    , fill_color=\"red\"\n",
    "    , stroke_color=\"red\"\n",
    ")\n",
    "\n",
    "lines_layer = gmaps.drawing_layer(features=[\n",
    "    gmaps.Line(\n",
    "          start = (path['Lat_x'], path['Lon_x'])\n",
    "        , end   = (path['Lat_y'], path['Lon_y'])\n",
    "        , stroke_weight=2\n",
    "        , stroke_color=\"red\"\n",
    "    )\n",
    "    for path in paths_host.to_dict('records')]\n",
    ")\n",
    "\n",
    "fig = gmaps.figure(layout={'height': '500px'})\n",
    "fig.add_layer(parking_layer)\n",
    "fig.add_layer(destinations_layer)\n",
    "fig.add_layer(lines_layer)\n",
    "embed_minimal_html('maps_rendered/map_walk_final.html', views=[fig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
