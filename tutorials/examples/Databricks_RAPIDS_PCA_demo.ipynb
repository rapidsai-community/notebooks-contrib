{"cells":[{"cell_type":"markdown","source":["#GPU Accelerated Principal Component Analysis (PCA) using RAPIDS on a Sample Dataset with CPU vs GPU comparison"],"metadata":{}},{"cell_type":"markdown","source":["#### Verifying GPUs\n\nRAPIDS requires GPUs with Pascal Architecture or better. That means any GPUs starting with K (Kepler) series (e.g. K80) or M (Maxwell) will not work with RAPIDS. You can use the `nvidia-smi` command to verify the type of your GPU as well as the memory size which may be needed for some of the RAPIDS examples."],"metadata":{}},{"cell_type":"code","source":["%sh nvidia-smi"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Let's begin by importing RAPIDS and scikit learn libraries!"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA as skPCA\nfrom cuml import PCA as cumlPCA\nimport cudf\nimport os"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Downloading the data\nFor this example we are downloading a sample dataset (Mortgage.csv) from Nvidia's repository\n\n__We have already completed Data prep (ETL) and feature engineering on this dataset and the dataset is ready for Machine Learning__\n\nWe're going to first visually check the directory to see if you already have the dataset __\"mortgage_data.avro__\" because it takes some time to download."],"metadata":{}},{"cell_type":"code","source":["%sh\nls /dbfs/RAPIDS/mortgage"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["__Note:__ If you already have a dataset, please go to the __\"Loading the data with Spark\" section__.  If you __don't__ have the dataset, please run the following commands:"],"metadata":{}},{"cell_type":"code","source":["%sh \n\nwget https://github.com/rapidsai/notebooks-extended/raw/master/data/mortgage_data_tar.tar.gz\n\ntar -xzf mortgage_data_tar.tar.gz"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Check that everything downloaded and extracted ok"],"metadata":{}},{"cell_type":"code","source":["%sh\n\nls "],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Make a directory and copy the extracted file there."],"metadata":{}},{"cell_type":"code","source":["%sh\n\nmkdir -p /dbfs/RAPIDS/mortgage\nrm -rf /dbfs/RAPIDS/mortgage/mortgage_data.avro\nmv mortgage_data.avro /dbfs/RAPIDS/mortgage/mortgage_data.avro"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Loading the data with Spark"],"metadata":{}},{"cell_type":"code","source":["data = spark.read.format(\"avro\").load(\"/RAPIDS/mortgage/mortgage_data.avro/\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["__Helper Functions to remove any null values__"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\n\ndef recast(df):\n   for column, data_type in df.dtypes:\n       if str(data_type) == \"string\":\n           df = df.withColumn(column, df[column].cast(\"float\"))\n   return df\n\ndata = recast(data).fillna(-1)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["__Let's check out the dataset in the spark dataframe and count the number of rows in the dataset__"],"metadata":{}},{"cell_type":"code","source":["display(data)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["dataCount = data.count() # We're storing this value for later use\nprint(dataCount)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["__Helper Functions to compare CPU vs GPU results__"],"metadata":{}},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error\ndef array_equal(a,b,threshold=2e-3,with_sign=True):\n    a = to_nparray(a)\n    b = to_nparray(b)\n    if with_sign == False:\n        a,b = np.abs(a),np.abs(b)\n    error = mean_squared_error(a,b)\n    res = error<threshold\n    return res\n\ndef to_nparray(x):\n    if isinstance(x,np.ndarray) or isinstance(x,pd.DataFrame):\n        return np.array(x)\n    elif isinstance(x,np.float64):\n        return np.array([x])\n    elif isinstance(x,cudf.DataFrame) or isinstance(x,cudf.Series):\n        return x.to_pandas().values\n    return x    "],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Converting the Spark Dataframe into Pandas Dataframe"],"metadata":{}},{"cell_type":"markdown","source":["__Load data function allows you to create a user defined sample of your data and converts the spark dataframe to pandas dataframe.  Then, it removes any null values in the dataset.  If you want to to experiment with a different dataset sizes, use the random array generator to load the random data.__"],"metadata":{}},{"cell_type":"code","source":["def load_data(nrows, ncols):\n  try:\n    frac = nrows/dataCount # as sample() takes an integer, we are creating a factor by which to get the approximate number of rows \n    print(frac) # just for checks :)\n    if (frac > 1): \n      frac = 1.0\n    print(frac) # just for checks++ :)\n    X = data.sample(True, frac) \n    print(X)\n    df = X.toPandas() # we then convert the Spark Dataframe to Pandas.  \n    print(\"everything worked\")\n  except Exception as e: \n    print(e)\n    print('use random data')\n    X = np.random.rand(nrows,ncols)\n    df = pd.DataFrame({'fea%d'%i:X[:,i] for i in range(X.shape[1])})\n    print(\"only random data\")\n  return df"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["__Setting up data in Pandas Dataframe using Load data and null workaround function__"],"metadata":{}},{"cell_type":"code","source":["%%time\nnrows = 2**20\nnrows = int(nrows * 1.5)\nncols = 400\n\nX = load_data(nrows,ncols)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["# Brief Intro to PCA parameters"],"metadata":{}},{"cell_type":"markdown","source":["Let's take a look into all possible parameters that we can use when applying PCA: \nhttp://scikitlearn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n\nWe will start here with the following :\n\n__n_components__ : int, float, None or string  \nNumber of components to keep. if n_components is not set all components are kept\n\n__whiten__ : bool, optional (default False) \nWhen True (False by default) the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances. Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n\n__random_state__ : int, RandomState instance or None, optional (default None) \nIf int, random_state is the seed used by the random number generator\n\n__svd_solver__ : string {‘auto’, ‘full’, ‘arpack’, ‘randomized’} \nIf \"full\" :run exact full SVD calling the standard LAPACK solver via scipy.linalg.svd and select the components by postprocessing"],"metadata":{}},{"cell_type":"code","source":["n_components = 10\nwhiten = False\nrandom_state = 42\nsvd_solver=\"full\"\n"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["# Run PCA on CPU"],"metadata":{}},{"cell_type":"markdown","source":["Let's check the time needed to execute PCA function using standard sklearn library. \n__Note: this algorithm runs on CPU only.__"],"metadata":{}},{"cell_type":"code","source":["import multiprocessing\nprint(multiprocessing.cpu_count()) # Return the number of CPUs in the system."],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["%%time\npca_sk = skPCA(n_components=n_components,svd_solver=svd_solver, \n            whiten=whiten, random_state=random_state)\nresult_sk = pca_sk.fit_transform(X)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["# Run PCA on GPU"],"metadata":{}},{"cell_type":"markdown","source":["Now, before we execute PCA function using RAPIDS cuml library we will first read the data in GPU data format using cudf. \n\n__cudf__ - GPU DataFrame manipulation library https://github.com/rapidsai/cudf\n\n__cuml__ - suite of libraries that implements a machine learning algorithms within the RAPIDS data science ecosystem https://github.com/rapidsai/cuml"],"metadata":{}},{"cell_type":"code","source":["Xt = cudf.DataFrame.from_pandas(X) # Convert Pandas Dataframe to GPU Dataframe!"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%%time\npca_cuml = cumlPCA(n_components=n_components,svd_solver=svd_solver, \n            whiten=whiten, random_state=random_state)\nresult_cuml = pca_cuml.fit_transform(Xt)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["for attr in ['singular_values_','components_','explained_variance_',\n             'explained_variance_ratio_']:\n    passed = array_equal(getattr(pca_sk,attr),getattr(pca_cuml,attr))\n    message = 'compare pca: cuml vs sklearn {:>25} {}'.format(attr,'equal' if passed else 'NOT equal')\n    print(message)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# Spark ML accelerated with RAPIDS\npassed = array_equal(result_sk,result_cuml)\nmessage = 'compare pca: cuml vs sklearn transformed results %s'%('equal'if passed else 'NOT equal')\nprint(message)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":41}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.6","nbconvert_exporter":"python","file_extension":".py"},"name":"Databricks_RAPIDS_PCA_demo","notebookId":3941319872928235},"nbformat":4,"nbformat_minor":0}
