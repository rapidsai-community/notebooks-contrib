{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NRHktCLRGm1z"
   },
   "source": [
    "# Density-Based Spatial Culstering of Applications with Noise (DBSCAN)\n",
    "\n",
    "The DBSCAN algorithm is a clustering algorithm which works really well for datasets in which samples conregate in large groups. cuMLâ€™s DBSCAN expects a cuDF DataFrame, and constructs an adjacency graph to compute the distances between close neighbours. The DBSCAN model implemented in the cuML library can accept the following parameters :\n",
    "- `eps`: maximum distance between 2 sample points \n",
    "- `min_samples`: minimum number of samples that should be present in a neighborhood for it to be considered as a core points\n",
    "\n",
    "The methods that can be used with DBSCAN are: \n",
    "- `fit`: Perform DBSCAN clustering from features\n",
    "- fit_predict: Performs clustering on input_gdf and returns cluster labels\n",
    "- `get_params`: Sklearn style return parameter state\n",
    "- `set_params`: Sklearn style set parameter state to dictionary of params\n",
    "\n",
    " \n",
    "The model accepts only numpy arrays or cudf dataframes as the input. \n",
    "  - In order to convert your dataset to cudf format please read the cudf [documentation](https://docs.rapids.ai/api/cudf/stable/) \n",
    "  - For additional information on the DBSCAN model please refer to the [documentation](https://rapidsai.github.io/projects/cuml/en/stable/api.html#dbscan) </p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xsOMscit-OD"
   },
   "source": [
    "# Setup\n",
    "\n",
    "1. Ensure that you have selected Python3 as your runtime type and 'GPU' as your hardware accelerator from the menu: Runtime > Change Runtime Type.\n",
    "2. Use pynvml to confirm Colab allocated you a Tesla T4 GPU.\n",
    "3. Install most recent Miniconda release compatible with Google Colab's Python install (3.6.7).\n",
    "4. Install RAPIDS libraries.\n",
    "5. Copy RAPIDS .so files into current working directory, a workaround for conda/colab interactions.\n",
    "6. Update env variables so Python can find and use RAPIDS artifacts.\n",
    "7. All of the above steps are automated in the next cell.\n",
    "8. You should re-run this cell any time your instance re-starts.\n",
    "    - may take a few minutes\n",
    "    - long output (output display removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRtYd8jAtuE1"
   },
   "outputs": [],
   "source": [
    "!wget -nc https://github.com/rapidsai/notebooks-extended/raw/master/utils/rapids-colab.sh\n",
    "!bash rapids-colab.sh\n",
    "\n",
    "import sys, os\n",
    "\n",
    "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
    "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
    "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# rapids\n",
    "import cudf\n",
    "from cuml import DBSCAN as cumlDBSCAN\n",
    "from sklearn.cluster import DBSCAN as skDBSCAN\n",
    "# dask\n",
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FB45vOmTNBJF"
   },
   "source": [
    "## Data\n",
    "\n",
    "Here we can utilize either of the two load data functions:\n",
    "1. Loading data from the zipped file into regular dataframe\n",
    "2. Loading data from the zipped file into a CUDA dataframe\n",
    "    - NOTE: The following functions both provide the same end result (a pandas dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VApzmf9pvbNh"
   },
   "outputs": [],
   "source": [
    "def load_data(nrows, ncols, cached = 'data/mortgage.npy.gz'):\n",
    "  if os.path.exists(cached):\n",
    "      print('Using Mortgage Data')\n",
    "      with gzip.open(cached) as f:\n",
    "          X = np.load(f)\n",
    "      X = X[np.random.randint(0,X.shape[0]-1,nrows),:ncols]\n",
    "  else:\n",
    "      # create a random dataset\n",
    "      print('Using Random Data')\n",
    "      X = np.random.rand(nrows,ncols)\n",
    "  df = pd.DataFrame({'fea%d'%i:X[:,i] for i in range(X.shape[1])})\n",
    "  return df\n",
    "\n",
    "def load_data_alternate(nrows, ncols, cached = 'data/mortgage.csv.gz'):\n",
    "  if os.path.exists(cached):\n",
    "    with gzip.open(cached) as f:\n",
    "      print('Using Mortgage Data')\n",
    "      X = cudf.read_csv(f, usecols=[i for i in range(0, ncols)], nrows=nrows, header=None)\n",
    "      return X\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_iOGAARKIgR4",
    "outputId": "9d130ed2-f5a5-4d62-fa0a-a8cb32b874eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Random Data\n"
     ]
    }
   ],
   "source": [
    "# Setting the  number of rows and columns that will be imported.\n",
    "# Play around with the numbers: let nrows be (500, 5000) and run tests.\n",
    "nrows = 10000\n",
    "ncols = 20\n",
    "df = load_data(nrows, ncols)\n",
    "#df = load_data_alternate(nrows, ncols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdGAVt_v754j"
   },
   "source": [
    "## Performing Clustering\n",
    "\n",
    "Setting up variables for distance between 2 sample points and the minimum number of samples for the DBSCAN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQRpLFzP8LTY"
   },
   "outputs": [],
   "source": [
    "# eps = maximum distance between 2 sample points for them to be in the same neighborhood\n",
    "# min_samples = number of samples that should be present in a neighborhood for it to be considered as a core point\n",
    "\n",
    "eps = 3\n",
    "min_samples = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "777wCl15-GiF"
   },
   "source": [
    "**<p> At this point, we can now compare the performance between the traditional sklearn dbscan model and the implementation done utilizing CUDA. </p>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "_MFt-o478Mhr",
    "outputId": "70bd53d8-b5ab-477c-d401-a47522db4a22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.72 s, sys: 1.29 s, total: 7.02 s\n",
      "Wall time: 7.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use the sklearn DBSCAN model to fit the dataset \n",
    "clustering_sk = skDBSCAN(eps = eps, min_samples = min_samples)\n",
    "clustering_sk.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqd-U-wDpVJP"
   },
   "outputs": [],
   "source": [
    "# convert dataframe to cudf from pandas \n",
    "df = cudf.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "iXi6SI3y8Mvu",
    "outputId": "8fdd69e5-7a4b-43e2-c129-2e6a0af97885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 124 ms, total: 1.22 s\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run the cuml DBSCAN model to fit the dataset \n",
    "clustering_cuml = cumlDBSCAN(eps = eps, min_samples = min_samples)\n",
    "clustering_cuml.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8USwaCjrt2A"
   },
   "source": [
    "**<p>These two functions determine whether the results from cuml and sklearn are equivalent.</p>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0ovJ2j-rcKn"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# the function converts a variable from ndarray or dataframe format to numpy array\n",
    "def to_nparray(x):\n",
    "    if isinstance(x,np.ndarray) or isinstance(x,pd.DataFrame):\n",
    "        return np.array(x)\n",
    "    elif isinstance(x,np.float64):\n",
    "        return np.array([x])\n",
    "    elif isinstance(x,cudf.DataFrame) or isinstance(x,cudf.Series):\n",
    "        return x.to_pandas().values\n",
    "    return x\n",
    "\n",
    "def array_equal(a,b,threshold=5e-3,with_sign=True):\n",
    "    a = to_nparray(a)\n",
    "    b = to_nparray(b)\n",
    "    if with_sign == False:\n",
    "        a,b = np.abs(a),np.abs(b)\n",
    "    error = mean_squared_error(a,b)\n",
    "    res = error<threshold\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKD8-SADmjP_"
   },
   "source": [
    "**<p>Ensuring that the results from both methods give the same output.</p>**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Nqv9d1B8mi4B",
    "outputId": "639950f7-aa21-41d9-fca4-54afa4e38299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are equal.\n"
     ]
    }
   ],
   "source": [
    "equals = array_equal(clustering_cuml.labels_,clustering_sk.labels_)\n",
    "if equals:\n",
    "  print(\"Results are equal.\")\n",
    "else:\n",
    "  print(\"Results are not equal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oCR9M6xXJXrL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DBScan.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
