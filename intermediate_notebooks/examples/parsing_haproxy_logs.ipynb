{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HAProxy Logs\n",
    "\n",
    "This notebook demonstrates real world log parsing examples. For this notebook we will use the popular HAProxy\n",
    "format since that is a common realworld use case.\n",
    "\n",
    "This notebook will cover\n",
    "+ Streaming log entry event level logs from Kafka using Streamz\n",
    " - Those interested in Kafka can refer to: https://kafka.apache.org/quickstart to start a local Kafka cluster.\n",
    " - Also from a text file in case Kafka is not present to user.\n",
    "+ Counting log entries - (Word Count)\n",
    "+ Counting log entries from a file (no Kafka)\n",
    "+ Calculating average backend response time\n",
    "\n",
    "Ok so what do these HAProxy logs look like? Well here is an example.\n",
    "\n",
    "```{\"logline\": \"[haproxy@10.0.0.1] <134>May 29 19:08:36 haproxy[113498]: 45.26.605.15:38738 [29/May/2019:19:08:36.691] HTTPS:443~ HTTP_ProvisionManagers/mp3 4/5/0/1/1 200 6182 - - --NI 3/3/0/0/0 0/0 {|} \"GET /v2/serverinfo HTTP/1.1\"}```\n",
    "\n",
    "It is unlikely you have a Kafka topic with these log messages in it already so lets generate some sample messages for you. First we need to install a few required dependencies and define some global configurations however.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.6.14\n",
      "  latest version: 4.7.12\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge -y streamz python-confluent-kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import confluent_kafka as ck\n",
    "\n",
    "num_messages_to_produce = 100000\n",
    "\n",
    "kafka_brokers = ['localhost:9092'] # This is a list of your Kafka brokers and ports.\n",
    "topic = 'haproxy-logs'\n",
    "\n",
    "kafka_conf = {'bootstrap.servers': kafka_brokers, 'compression.type':'snappy', 'group.id': 'custreamz', 'session.timeout.ms': 60000}  # Kafka configuration parameters. Any additional Kafka configurations can be placed here ...\n",
    "\n",
    "producer = ck.Producer(kafka_conf)  # Kafka producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sample HAProxy Data\n",
    "First thing is first. Lets generate some sample HAProxy logs into our Kafka environment so the following examples have something to pull from. This code has nothing to do with RAPIDS but rather just a simple script for generating sample HAProxy logs and publishing them into Kafka.\n",
    "\n",
    "This is certainly not the most efficient way to write to Kafka but it is the most simple for example purposes. Please be patient while the produce occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import randrange\n",
    "\n",
    "sample_data = {}\n",
    "sample_data['log_ip'] = ['10.0.0.3', '14.2.3.4', '15.2.6.9', '10.2.34.6', '10.23.34.1']\n",
    "sample_data['syslog_timestamp'] = ['May 28 2019 00:00:09', 'May 28 2019 00:00:10', 'May 28 2019 00:00:11',\\\n",
    "                                   'May 28 2019 00:00:39','May 28 2019 00:00:51', 'May 28 2019 00:10:09']\n",
    "sample_data['program'] = ['haproxy']\n",
    "sample_data['pid'] = [113345, 756487, 352453, 352465, 164541]\n",
    "sample_data['client_ip'] = ['156.23.224.56', '126.52.74.15', '247.81.56.21', '26.245.255.1', '255.116.145.2']\n",
    "sample_data['client_port'] = [13345, 56487, 52453, 52465, 64541]\n",
    "sample_data['accept_date'] = ['28/May/2019:00:10:09.492', '28/May/2019:00:09:10.006', '28/May/2019:00:02:10.748',\\\n",
    "                              '28/May/2019:00:20:10.891', '28/May/2019:00:02:10.461', '28/May/2019:00:02:11.959']\n",
    "sample_data['frontend_name'] = ['px-http', 'https:443', 'tx-http']\n",
    "sample_data['server_name'] = ['srv1', 'srv2', 'srv3', 'srv4', 'srv5']\n",
    "sample_data['time_request'] = [0, 1, 2, 3]\n",
    "sample_data['time_queue'] = [0, 1, 2, 3]\n",
    "sample_data['time_backend_connect'] = [1, 2, 3]\n",
    "sample_data['time_backend_response'] = [2, 3, 4, 5, 6, 7, 8, 9]\n",
    "sample_data['time_duration'] = [13, 14, 16, 20, 23, 25]\n",
    "sample_data['http_status_code'] = [200, 400, 201, 401, 403]\n",
    "sample_data['bytes_read'] = [4, 573, 442, 234, 124, 1567]\n",
    "sample_data['captured_request'] = ['-']\n",
    "sample_data['captured_response'] = ['-']\n",
    "sample_data['termination_state'] = ['----', 'PH--', 'CR--', '--NI', '--SG']\n",
    "sample_data['actconn'] = [1, 2, 3, 4]\n",
    "sample_data['feconn'] = [2, 3, 5, 7, 8]\n",
    "sample_data['beconn'] = [0, 1, 2, 3, 4]\n",
    "sample_data['srvconn'] = [0, 1, 3]\n",
    "sample_data['retries'] = [0, 1, 2]\n",
    "sample_data['srv_queue'] = [0, 1, 2, 3]\n",
    "sample_data['backend_queue'] = [0, 2, 3, 4, 5, 7, 8, 9]\n",
    "\n",
    "cols = ['log_ip','syslog_timestamp','program','pid','client_ip','client_port',\\\n",
    "        'accept_date','frontend_name','backend_name','server_name','time_request',\\\n",
    "        'time_queue','time_backend_connect', 'time_backend_response', 'time_duration',\\\n",
    "        'http_status_code', 'bytes_read', 'captured_request', 'captured_response',\\\n",
    "        'termination_state','actconn','feconn','beconn','srvconn','retries','srv_queue','backend_queue']\n",
    "\n",
    "def generate_log():\n",
    "    log_skelton = \"[haproxy@{0}] <134>{1} {2}[{3}]: {4}:{5} [{6}] {7} {8}/{9} {10}/{11}/{12}/{13}/{14} {15} {16} {17} {18} {19} {20}/{21}/{22}/{23}/{24} {25}/{26}\"\n",
    "    values = []\n",
    "    for idx, col in enumerate(cols):\n",
    "        if col in sample_data:\n",
    "            value_list = sample_data[col]\n",
    "            values.append(value_list[randrange(len(value_list))])\n",
    "        else:\n",
    "            values.append(values[-1])\n",
    "    dict_out = {}    \n",
    "    dict_out[\"logline\"] = log_skelton.format(*values)\n",
    "    return json.dumps(dict_out)\n",
    "\n",
    "\n",
    "count = 0\n",
    "try:\n",
    "    while count < num_messages_to_produce:\n",
    "        producer.produce(topic, generate_log())\n",
    "        count = count + 1\n",
    "except KeyboardInterrupt:\n",
    "    sys.stderr.write('%% Aborted by user\\n')\n",
    "    \n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Log Entries\n",
    "\n",
    "Let's assume that the data coming in to the Kafka topic â€” i.e., each record/message, is a line in the form of \"this is line x\", where x is an incremental counter. \n",
    "    \n",
    "Now, we write a function to parse each such message to get the list of words in each line. \n",
    "\n",
    "One can also make use of nvstrings (now custrings, the GPU-accelerated string manipulation library) to tokenise all the messages in the batch. Refer to process_batch_nvstrings()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streamz and cudf imports\n",
    "import cudf\n",
    "from streamz import Stream\n",
    "from streamz.dataframe import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "#Helper function operating on every batch polled from Kafka for word count\n",
    "def process_batch(messages):\n",
    "    y = []\n",
    "    for x in messages:\n",
    "        y = y + x.decode('utf-8').strip('\\n').split(\" \")\n",
    "    return y\n",
    "\n",
    "import nvstrings, nvtext\n",
    "def process_batch_nvstrings(messages):\n",
    "    messages_decoded = []\n",
    "    for message in messages:\n",
    "        messages_decoded.append(message.decode('utf-8'))\n",
    "    device_lines = nvstrings.to_device(messages_decoded)\n",
    "    words = nvtext.tokenize(device_lines)\n",
    "    return words\n",
    "\n",
    "\n",
    "# We now use Streamz to create a Stream from Kafka by polling the topic every 10s. If you changed Dask=True, please ensure you have a Dask cluster up and running\n",
    "source = Stream.from_kafka_batched(topic, kafka_conf, npartitions=1, poll_interval='10s', asynchronous=True, dask=False)\n",
    "\n",
    "#Applying process_batch function to process word count on each batch\n",
    "stream = source.map(process_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Streamz DataFrame does the trick!* \n",
    "\n",
    "After we get the parsed word list on our stream from Kafka, we just perform simple aggregations using the Streamz DataFrame to get the word count.\n",
    "\n",
    "We then write the output (word count) to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = stream.map(lambda words: cudf.DataFrame({'word': words, 'count': np.ones(len(words),dtype=np.int32)}))\n",
    "sdf = DataFrame(stream_df, example=cudf.DataFrame({'word':[], 'count':[]}))\n",
    "output = sdf.groupby('word').sum().stream.buffer(100000).gather().sink_to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the stream!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what output we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have cuDF dataframe that got produced to the output. Let's see if we can print some actual word counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the values\n",
    "print(output[0].loc[65:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Log Entries From File\n",
    "\n",
    "For this example, we will be demonstrating how to stream from a textfile. Please install pytest using conda to use the tmpfile() function from streamz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streamz and cudf imports\n",
    "import cudf\n",
    "from streamz import Stream\n",
    "from streamz.dataframe import DataFrame\n",
    "from streamz.utils_test import tmpfile\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the data coming in to a textfile and each line is in the form of \"this is line x\", where x is an incremental counter.\n",
    "\n",
    "Now, we write a function to parse each line to get the list of words in each line.\n",
    "\n",
    "One can also make use of nvstrings (now custrings, the GPU-accelerated string manipulation library) to tokenise each line. Refer to process_line_nvstrings()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    words = line.strip('\\n').split(\" \")\n",
    "    return words\n",
    "\n",
    "import nvstrings, nvtext\n",
    "def process_line_nvstrings(line):\n",
    "    device_line = nvstrings.to_device(line)\n",
    "    words = nvtext.tokenize(device_line)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a temporary textfile using tmpfile() which streamz.utils_test provides to simulate streaming word count from a textfile.\n",
    "\n",
    "*One can write a separate function to write continuously to a textfile, and still use the same cuStreamz code as shown below to calculate word count.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tmpfile() as fn:\n",
    "    with open(fn, 'wt') as f:\n",
    "        #Write some random data to the file\n",
    "        for i in range(0,10):\n",
    "            f.write(\"this is line \" + str(i) + \"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        #Create a stream from the textfile, and specify the interval to poll the file at.\n",
    "        source = Stream.from_textfile(fn, poll_interval=0.01, \\\n",
    "                                 asynchronous=True, start=False)\n",
    "        \n",
    "        #Apply the process_line helper function on each element/line streamed from the textfile.\n",
    "        stream = source.map(process_line)\n",
    "        \n",
    "        '''\n",
    "        Streamz DataFrame does the trick!\n",
    "        \n",
    "        After we get the parsed word list on our stream from the textfile, \n",
    "        we just perform simple aggregations using the Streamz DataFrame to get the word count.\n",
    "        \n",
    "        We then write the output (word count) to a list.\n",
    "        '''\n",
    "        stream_df = stream.map(lambda words: cudf.DataFrame({'word': words, 'count': np.ones(len(words),dtype=np.int32)}))\n",
    "        sdf = DataFrame(stream_df, example=cudf.DataFrame({'word':[], 'count':[]}))\n",
    "        output = sdf.groupby('word').sum().stream.gather().sink_to_list()\n",
    "        \n",
    "        #Starting the stream!\n",
    "        source.start()\n",
    "        \n",
    "        time.sleep(2)\n",
    "        '''\n",
    "        We can see that we have cuDF dataframe that got produced to the output. \n",
    "        Let's see if we can print some actual word counts.\n",
    "        ''' \n",
    "        print(output[-1].loc[9:])\n",
    "        \n",
    "        '''\n",
    "        We can! :)\n",
    "\n",
    "        Now, we write some more data to the text file and wait for some more time before checking the output again.\n",
    "\n",
    "        If we're sure of what's happening, the output should now have a list of cuDF dataframes, \n",
    "        each having the cumulative streaming word count of all the data seen until now, \n",
    "        the last cuDF dataframe being the most recent.\n",
    "        '''\n",
    "        #Write more random data to the file\n",
    "        for i in range(10,20):\n",
    "            f.write(\"this is line \" + str(i) + \"\\n\")\n",
    "        f.flush()\n",
    "        \n",
    "        time.sleep(2)\n",
    "        print(output[-1].loc[9:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Average Backend Response Time\n",
    "\n",
    "Below is a helper function which implements parsing on the HAProxy logs, and then calculates the average backend response time for each batch.\n",
    "\n",
    "It also has timestamps to determine the time taken by each important phase of the stream processing â€” parsing and aggregations. These times are returned along with the average backend response time taken for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streamz and cudf imports\n",
    "from streamz import Stream\n",
    "import cudf\n",
    "from streamz.dataframe import DataFrame\n",
    "import time\n",
    "\n",
    "def haproxy_parsing_aggregations(messages):\n",
    "    \n",
    "    preprocess_start_time = int(round(time.time()))\n",
    "    \n",
    "    size = len(messages)*len(messages[0]) \n",
    "    num_rows = len(messages)\n",
    "    json_input_string = \"\\n\".join([msg.decode('utf-8') for msg in messages])\n",
    "    \n",
    "    gdf = cudf.read_json(json_input_string, lines=True, engine='cudf')\n",
    "    \n",
    "    pre_parsing_time = int(round(time.time()))\n",
    "    \n",
    "    '''\n",
    "    Piecemeal log parsing for HAProxy\n",
    "    '''\n",
    "    \n",
    "    clean_df = gdf['logline'].str.split(' ')\n",
    "\n",
    "    clean_df['log_ip'] = clean_df[0].str.lstrip('[haproxy@').str.rstrip(']')\n",
    "    clean_df.drop_column(0)\n",
    "\n",
    "    clean_df[1] = clean_df[1].str.split('>')[1]\n",
    "    syslog_timestamp = clean_df[1].data.cat([clean_df[2].data, clean_df[3].data, clean_df[4].data], sep=' ')\n",
    "    clean_df['syslog_timestamp'] = cudf.Series(syslog_timestamp)\n",
    "    for col in [1,2,3,4]:\n",
    "        clean_df.drop_column(col)\n",
    "\n",
    "    program_pid_df = clean_df[5].str.split('[')\n",
    "    program_sr = program_pid_df[0]\n",
    "    pid_sr = program_pid_df[1]\n",
    "    clean_df['program'] = program_sr\n",
    "    clean_df['pid'] = pid_sr.str.rstrip(']:')\n",
    "    clean_df = clean_df.drop(labels=[5])\n",
    "    del program_pid_df\n",
    "\n",
    "    client_pid_port_df = clean_df[6].str.split(':')\n",
    "    clean_df['client_ip'], clean_df['client_port'] = client_pid_port_df[0], client_pid_port_df[1]\n",
    "    clean_df.drop_column(6)\n",
    "    del client_pid_port_df\n",
    "\n",
    "    clean_df['accept_date'] = clean_df[7].str.lstrip('[').str.rstrip(']')\n",
    "    clean_df.drop_column(7)\n",
    "\n",
    "    clean_df.rename({8: 'frontend_name'}, inplace=True)\n",
    "    backend_server_df = clean_df[9].str.split('/')\n",
    "    clean_df['backend_name'], clean_df['server_name'] = backend_server_df[0], backend_server_df[1]\n",
    "    clean_df.drop_column(9)\n",
    "\n",
    "    time_cols = ['time_request', 'time_queue', 'time_backend_connect', 'time_backend_response', 'time_duration']\n",
    "    time_df = clean_df[10].str.split('/')\n",
    "    for col_id, col_name in enumerate(time_cols):\n",
    "        clean_df[col_name] = time_df[col_id]\n",
    "    clean_df.drop_column(10)\n",
    "    del time_df\n",
    "\n",
    "    clean_df.rename({11: 'http_status_code'}, inplace=True)\n",
    "    clean_df.rename({12: 'bytes_read'}, inplace=True)\n",
    "    clean_df.rename({13: 'captured_request', 14: 'captured_response', 15: 'termination_state'}, inplace=True)\n",
    "\n",
    "    con_cols = ['actconn', 'feconn', 'beconn', 'srvconn', 'retries']\n",
    "    con_df = clean_df[16].str.split('/')\n",
    "    for col_id, col_name in enumerate(con_cols):\n",
    "        clean_df[col_name] = con_df[col_id]\n",
    "    clean_df.drop_column(16)\n",
    "    del con_df\n",
    "\n",
    "    q_df = clean_df[17].str.split('/')\n",
    "    clean_df['srv_queue'], clean_df['backend_queue'] = q_df[0], q_df[1]\n",
    "    clean_df.drop_column(17)\n",
    "    del q_df\n",
    "    \n",
    "    post_parsing_time = int(round(time.time()))\n",
    "    \n",
    "    '''\n",
    "    End of the piecemeal log parsing for HAProxy.\n",
    "    Simple aggregations to be performed now.\n",
    "    '''\n",
    "    \n",
    "    clean_df['time_backend_response'] = clean_df['time_backend_response'].astype('int')\n",
    "    avg_backend_response_time = clean_df['time_backend_response'].mean()\n",
    "    \n",
    "    post_agg_time = int(round(time.time()))\n",
    "    \n",
    "    return \"{0},{1},{2},{3},{4},{5},{6}\".format(num_rows, preprocess_start_time, pre_parsing_time, \\\n",
    "                                            post_parsing_time, post_agg_time, \\\n",
    "                                            avg_backend_response_time, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you changed Dask=True, please ensure you have a Dask cluster up and running\n",
    "stream = Stream.from_kafka_batched(topic, kafka_conf, poll_interval='10s',\n",
    "                                   npartitions=1, asynchronous=True, dask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the helper parsing+aggregations function to perform the required operations on each batch polled from Kafka, and write the result into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = stream.map(haproxy_parsing_aggregations).buffer(100000).gather().sink_to_list()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lets start the stream!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
